---
title: "Colocation Walkthrough"
output: html_notebook
---

# Overview 

This is a complete walkthrough of the colocation analysis. We will import and clean QuantAQ and reference sensors. Then, we will align the datasets and analysis the trends between them.

# Downloading and Importing 

### Reference Instrument Data 

The reference instrument data consists of two parts : CPC and aethlometer data. Both datasets can be downloaded from [this dropbox folder](https://www.dropbox.com/sh/e53t6f18m0jdhdq/AABfYYiqq3ChPqY80Gd_ZK0Ya?dl=0). Directly download the whole dropbox folder. This dropbox is updated with more data weekly, but we will touch on this point later. 

### QuantAQ Data 

In order to download QuantAQ data, we can walk through the initial walkthrough (initial_analysis_walkthrough.rmd). This will give us instructions on where to download the data, how to clean it, and how to export it. 

## Storing Data

* Store the Dropbox downloaded folder in the "/data/reference_data" subfolder of this folder. 

* The initial walkthrough will provide instructions on how to store the QuantAQ data.


## Importing Data

### Importing Necessary Libraries 

Same as in the initial walkthrough, our import and analysis relies on functions from a few packages. Below, we'll import the necessary packages. 

```{r, warning= FALSE, error=FALSE}
library(lubridate) #date and time functions 
library(data.table) #to use the data.table variable type
library(dplyr) #this library allows you to use the %>% operator
library(tidyr) #this library lets you use the complete function to account for time syncing
library(openair) #for plotting and analysis
```

### Importing QuantAQ Data 

For colocation, we're going to be using sensors 45 and 46.

```{r}
sn45 <- fread("./data/cleaned_sn45.csv", header = TRUE)

#sn46 <- fread("./data/sn46.csv", header = TRUE)
```

When you export the data, the datetime object becomes a string and the timezone changes to UTC, but the data stays the same. We can preserve the data itself, but change the format with the ymd_hms formatting function. We can add the timezone parameter to specify the current timezone of the data. 

```{r}
sprintf("Current first datetime entry, and it's class: %s, %s", sn45$date[1], typeof(sn45$date))
sn45$date <- ymd_hms(sn45$date, tz= "America/New_York")
#sn46$date <- ymd_hms(sn46$date, tz="America/New_York")

sprintf("Modified first datetime entry, and it's class: %s, %s", sn45$date[1], typeof(sn45$date))
```


### Importing Reference Instrument Data 


#### Importing CPC Data 

First we import the files themselves. Since there's a lot of files that all contribute to one dataframe, we're going to find all the CPC files and append them to one dataframe, as we read them in. 

All the CPC data are saved as TXT files to the CPC folder from the Dropbox files. We can find all the CPC data by finding all the txt files in that folder. 

```{r}
nm <- list.files(path="./data/reference_data/MCPC data", pattern="*.TXT", full.names = TRUE) # find CPC data
```

```{r}
list.files(all.files=TRUE)
```


Since we've identified which files are CPC data and saved them to a list, we can run through that list and import the dataframes one by one. In R, you can layer functions together. We'll do that in this case, by overlaying the read function with rbind. What this does is that immediately after a dataframe is imported, it's bounded by row to a dataframe. This way, when the data imports, it all goes to one place. 

```{r}
cpc_upload <- do.call(rbind, lapply(nm, function(x) read.delim(file=x, header = TRUE, skip = 13) ) )
```

If you'd like you can view the dataframe:

```{r}
View(cpc_upload)
```

In this walkthrough, we're only concerned with the variables "concent" (concentration), "fillcnt" (which tells you if the sensor is working correctly) and the datetime. Let's discard the rest of the variables (columns). 

```{r}
cpc_upload <- cpc_upload[ , c("X.YY.MM.DD", "HR.MN.SC", "concent", "fillcnt"  )]   ## removing unnecessary variables 
```

Finally, let's put the datetime in the right format. We're going to format the hour/minute/second column and the year/month/day columns separately, using lubridate functions. Then, we'll combine the vectors with the function "with". Since the timezone is already local, we can define it using the lubridate package "tz".

```{r}
cpc_upload$date <- with(cpc_upload, ymd(`X.YY.MM.DD`) + hms(`HR.MN.SC`)) ##formatting datetime

tz(cpc_upload$date) <- "America/New_York" # define timezone
```




#### Importing Aethlometer Data 

The aethlometer data import is very similar to the CPC data import. Again, we find all of the aethlometer data and put it into one dataframe.

```{r}
#import and clean AETH 
nm <- list.files(path="./data/reference_data/AETHLOMETER", all.files = TRUE, full.names = TRUE, pattern="*.csv")

aeth_data <- do.call(rbind, lapply(nm, function(x) fread(file=x) ) )

```


The variables we care about from this sensor are the datetimes, the status of the sensor, and all the black carbon concentration data. 
```{r}
aeth_data <- aeth_data[ , c( "Date / time local",  "Timezone offset (mins)" , "Date local (yyyy/MM/dd)", "Time local (hh:mm:ss)", "Status" ,  "UV BC1", "Blue BC1", "Green BC1", "Red BC1", "IR BC1")]
```


We also format the datetime vector. Since the date and time are already combined into one vector, we're just going to put it in the standard format using the ymd_hms format. Since the data is in local time, we'll define the timezone to EST/EDT as well. 

```{r}
aeth_data$date <- ymd_hms(aeth_data$`Date / time local`)   ## formatting date time 
tz(aeth_data$date) <- "America/New_York"
```


# Checking the Data 

In this section we'll review the reference data, before we clean and analysis it.

## QuantAQ 

We will not be testing QuantAQ data, since it was checked in the previous walkthrough.

## Negative, NA and zero data trends 

In this section, we'll count the number of negative, NA and zero data points and determine whether they are trends and their sources. 

### CPC 

```{r}
na_count <-sapply(cpc_upload[, c( "concent", "fillcnt" )], function(y) sum(length(which(is.na(y)))))

negative_count <-sapply(cpc_upload[, c( "concent", "fillcnt" )], function(y) length(which((y)<0)))

zero_count <-sapply(cpc_upload[, c( "concent", "fillcnt" )], function(y) length(which((y)==0)))

data.frame(na_count, negative_count, zero_count)
```


### Aeth 

```{r}
na_count <-sapply(aeth_data[, -c("Date / time local", "Timezone offset (mins)", "Time local (hh:mm:ss)", "date", "Date local (yyyy/MM/dd)")], function(y) sum(length(which(is.na(y)))))

negative_count <-sapply(aeth_data[, -c("Date / time local", "Timezone offset (mins)", "Time local (hh:mm:ss)", "date", "Date local (yyyy/MM/dd)")], function(y) length(which((y)<0)))

zero_count <-sapply(aeth_data[, -c("Date / time local", "Timezone offset (mins)", "Time local (hh:mm:ss)", "date", "Date local (yyyy/MM/dd)")], function(y) length(which((y)==0)))

data.frame(na_count, negative_count, zero_count)
```

So, there are all three types of values. We can compare this to what we know about the data: 
* 

We see that there is the same number of NA values for all the variables. Let's see when they happen. 

```{r}
cbind(aeth_data[which(is.na(aeth_data$`UV BC1`)),"date"], aeth_data[which(is.na(aeth_data$`Blue BC1`)),"date"] , aeth_data[which(is.na(aeth_data$`Green BC1`)),"date"] , aeth_data[which(is.na(aeth_data$`Red BC1`)),"date"],aeth_data[which(is.na(aeth_data$`IR BC1`)),"date"])
```

## More Testing of CPC Data

We know that CPC data should never equal 0, and that when fillcount is not equal to 0, there has been an interference in the system. Using these two variables as metrics, we can test how much of the data is reliable. 

```{r}

```


# Cleaning the Data

Based on the checks from the previous section, we will clean the data.

## QuantAQ 

We will not be cleaning the QuantAQ data in this walkthrough, since it was cleaned in the previous walkthrough. 

## CPC 

## Aeth 

# TIme Alignment 

Because we round the datetime variables to merge the datasets, there is the chance that data can be up to one minute shifted. Therefore, each data point's datetime can be up to one minute off. For peak-by-peak analysis of plumes, this can make a big difference. In this section, we will correct for the time shift, and align the peaks. 

## Subset 

It is easier to do this analysis for one day than a whole timeframe. Therefore, before we start aligning the peaks, we will create a subset of the data for the day 10/15. We're looking at 10/15, since it is the date we will be analyzing later. 

## Plot Data 

We'll manually shift the data, by the trend that we see. In order to know what the time shift is, we'll plot the data and visually inspect it. 

We'll plot the 10/15 data between 12 and 4 pm using openair timePlot.

## Shifting Data 

## Validate Time Shift 

### Plot same data 

### Corr plot 

### Corr plot for whole time series 

# Initial Analysis 






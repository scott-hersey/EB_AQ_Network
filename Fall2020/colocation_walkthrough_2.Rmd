---
title: "Colocation Walkthrough 2"
---

# Overview 

This is a complete walkthrough of the colocation analysis. We will import and clean QuantAQ and reference sensors. Then, we will align the datasets and analysis the trends between them.

# Downloading and Importing 

### Reference Instrument Data 

The reference instrument data consists of two parts : CPC and aethlometer data. Both datasets can be downloaded from [this dropbox folder](https://www.dropbox.com/sh/e53t6f18m0jdhdq/AABfYYiqq3ChPqY80Gd_ZK0Ya?dl=0). Directly download the whole dropbox folder. This dropbox is updated with more data weekly, but we will touch on this point later. 

### QuantAQ Data 

In order to download QuantAQ data, we can walk through the initial walkthrough (initial_analysis_walkthrough.rmd). This will give us instructions on where to download the data, how to clean it, and how to export it. 

## Storing Data

* Store the Dropbox downloaded folder in the "/data/reference_data" subfolder of this folder. 

* The initial walkthrough provides instructions on how to store the QuantAQ data.


## Importing Data

### Importing Necessary Libraries 

Same as in the initial walkthrough, our import and analysis relies on functions from a few packages. Below, we'll import the necessary packages. 

```{r, warning= FALSE, error=FALSE}
library(lubridate) #date and time functions 
library(data.table) #to use the data.table variable type
library(dplyr) #this library allows you to use the %>% operator
library(tidyr) #this library lets you use the complete function to account for time syncing
library(openair) #for plotting and analysis
library(readr)
library(ggplot2)
library(sjmisc)
```

### Importing QuantAQ Data 

For colocation, we're going to be using sensors 45 and 46.

```{r}
sn45 <- fread("./data/cleaned_sn45.csv", header = TRUE)

#sn46 <- fread("./data/sn46.csv", header = TRUE)
```

When you export the data, the datetime object becomes a string and the timezone changes to UTC, but the data stays the same. We can preserve the data itself, but change the format with the ymd_hms formatting function. We can add the timezone parameter to specify the current timezone of the data. 

```{r}
sprintf("Current first datetime entry, and it's class: %s, %s", sn45$date[1], typeof(sn45$date))
sn45$date <- ymd_hms(sn45$date, tz= "America/New_York")
#sn46$date <- ymd_hms(sn46$date, tz="America/New_York")
sprintf("Modified first datetime entry, and it's class: %s, %s", sn45$date[1], typeof(sn45$date))
```


### Importing Reference Instrument Data 


#### Importing CPC Data 

First we import the files themselves. Since there's a lot of files that all contribute to one dataframe, we're going to find all the CPC files and append them to one dataframe, as we read them in. 

All the CPC data are saved as TXT files to the CPC folder from the Dropbox files. We can find all the CPC data by finding all the txt files in that folder.

```{r}
nm <- list.files(path="./Fall2020/data/reference_data/MCPC data", pattern="*.TXT", full.names = TRUE)
```

Since we've identified which files are CPC data and saved them to a list, we can run through that list and import the dataframes one by one. In R, you can layer functions together. We'll do that in this case, by overlaying the read function with rbind. What this does is that immediately after a dataframe is imported, it's bounded by row to a dataframe. This way, when the data imports, it all goes to one place. 

```{r}
cpc_upload <- do.call(rbind, lapply(nm, function(x) read.delim(file=x, header = TRUE, skip = 13) ) )
```


Then we can further prepare the data by removing unnecessary variables and formatting the datetime object. For the latter, we're going to format the hour/minute/second column and the year/month/day columns separately, using lubridate functions. Then, we'll combine the vectors with the function "with". Since the timezone is already local, we can define it using the lubridate package "tz".

```{r}
cpc_upload <- cpc_upload[ , c("X.YY.MM.DD", "HR.MN.SC", "concent", "fillcnt"  )]   ## removing unnecessary variables 
cpc_upload$date <- with(cpc_upload, ymd(`X.YY.MM.DD`) + hms(`HR.MN.SC`)) ##formatting datetime
tz(cpc_upload$date) <- "America/New_York" # define timezone
cpc_upload2 <- cpc_upload #to avoid mistakes
```

#### Importing Aethlometer Data 

The aethlometer data import is very similar to the CPC data import. Again, we find all of the aethlometer data and put it into one dataframe.

```{r}
aeth_files <- list.files(path="./Fall2020/data/reference_data/AETHLOMETER", all.files = TRUE, full.names = TRUE, pattern="*.csv")
aeth_data <- do.call(rbind, lapply(aeth_files, function(x) fread(file=x) ) )
```

The variables we care about from this sensor are the datetimes, the status of the sensor, and all the black carbon concentration data. 

```{r}
aeth_data <- aeth_data[ , c( "Date / time local",  "Timezone offset (mins)" , "Date local (yyyy/MM/dd)", "Time local (hh:mm:ss)", "Status" ,  "UV BC1", "Blue BC1", "Green BC1", "Red BC1", "IR BC1")]
```


We also format the datetime vector. Since the date and time are already combined into one vector, we're just going to put it in the standard format using the ymd_hms format. Since the data is in local time, we'll define the timezone to EST/EDT as well. 

```{r}
aeth_data$date <- ymd_hms(aeth_data$`Date / time local`)   ## formatting date time 
tz(aeth_data$date) <- "America/New_York"
```

# Reformatting the data 

## Time offsets

The first thing we will change is to move the SN 45 date back four hours (the time offset given on the CPC data). 

```{r}
length(unique(aeth_data$`Timezone offset (mins)`)) == 1 #checking that I can just call the first offset, that all of the offsets are the same
sn45$date <- sn45$date + (aeth_data$`Timezone offset (mins)`[1] *60)
```

## QuantAQ 

We will not be further changing the QuantAQ data, since it was validated in the initial walkthrough.

## Vector Types 

We will check that the columns we expect to be numeric are not given as strings. Like in the HEPA analysis, we will check this with the typeof function. 

```{r}
sapply(list("cpc" = cpc_upload2, "aeth" = aeth_data), function(x) sapply(x,typeof))
```

Everything looks correct. 

## Checking and removing zero and negative values 

First, we'll check how many values in each dataset are negative or zero.

```{r}
sprintf("Number of negative and zero values in cpc: %d", sum(cpc_upload2$concent <= 0, na.rm = TRUE))
lapply(c( "UV BC1", "Blue BC1", "Green BC1", "Red BC1", "IR BC1"), function(x) sprintf("Number of negative and zero values in aethlometer data, %s : %d",x, sum(aeth_data[[x]] <= 0, na.rm = TRUE)))

sprintf("Percent of negative and zero values in cpc: %f", 100*sum(cpc_upload2$concent <= 0, na.rm = TRUE)/ nrow(cpc_upload2))
lapply(c( "UV BC1", "Blue BC1", "Green BC1", "Red BC1", "IR BC1"), function(x) sprintf("Percent of negative and zero values in aethlometer data, %s : %f",x, 100*sum(aeth_data[[x]] <= 0, na.rm = TRUE)/ nrow(aeth_data[[x]])))

```

These aren't high values, so we don't have to check the data, and can just set them to NA. 

```{r}
#set zero and negative cpc data to NA
cpc_upload2$concent[which(cpc_upload2$concent <= 0)] <- NA

#set zero and negative aethlometer data to NA
bc <- list("UV BC1", "Blue BC1" ,  "Green BC1" , "Red BC1" , "IR BC1" )   
for(col in bc){
  aeth_data[[col]] <- replace(aeth_data[[col]], aeth_data[[col]] <= 0, NA)
}
```

## Checking NA values

Let's make sure we don't have too many NA values 

```{r}
sprintf("Number of NA values in cpc: %d", sum(is.na(cpc_upload2$concent)))
sprintf("Percentage of NA values in cpc: %f", 100*sum(is.na(cpc_upload2$concent))/ nrow(cpc_upload2))
```

# Subsetting QuantAQ data

We'll subset the QuantAQ data so that it just contains data from the timeframe when the CPC and aethlometer trials were done. . In this case, that will be when there are only datapoints between the first date in the aethlometer data and the last date in the aethlometer data.

```{r}
sn45_subset <- sn45[date %between% c(aeth_data$date[1],tail(aeth_data$date, n=1))]
```

# Plotting 

## Define a color palette

In this walkthrough, I define a color palette for the graphs. I do this so I know what color corresponds to what variable without having to choose colors every time. Also, this color palette is supposed to more conducive toreading graphs with color blindness. 

We define a color palette just by creating a list with a bunch of colors in hex code (I think).

```{r}
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")
```

## Plotting BC and SN45 together 

The way we're going to plot in this walkthrough is by using ggplot and plotly. 

One way to plot with ggplot is to create a graph object. That's "object = ggplot()". Then, we say that we're going to a add a line to that graph. That's "geom_line". The parameters for geom_line are the dataset, and then the aesthetics, which are what the x and y data from that dataset are that we will be plotting. After that, we define the color. 

When we want to add more lines to the same graph, we literally add ("+") a new geom_line to the existing code. Note that the new lines can take data from different datasets. 

Next, we set our x-axis so it's titled "Date" and labels the dates that we see as "%b %d %I %p", which means month day and then the hour with either AM or PM. The ggplot page for scale_x_datetime contains the naming conventions for these formattings. 

Lastly, we're going to say we have a second y axis. In ggplot, you can't just say that you want a second axis based on another dataset and have it scale. You have to scale the data yourself. So, you have to say that you want the original y axis to be scaled by a certain amount (I'm saying that I'm scaling by 100 in this case). I'm also manually dividing the BC by 100 so that it looks to be on the same scale as the NO. 

```{r}
#create sn45 and BC plot
no_aeth <- ggplot() + 
  #plot sn45 data
  geom_line(data=sn45_subset, aes(x=date, y=no), color=cbPalette[1]) + 
  #plot BC data
  geom_line(data=aeth_data, aes(x=date, y=`Blue BC1`/100), color=cbPalette[2]) +
  geom_line(data=aeth_data, aes(x=date, y=`Red BC1`/100), color=cbPalette[3]) +
  geom_line(data=aeth_data, aes(x=date, y=`Green BC1`/100), color=cbPalette[4]) +
  geom_line(data=aeth_data, aes(x=date, y=`IR BC1`/100), color=cbPalette[5]) +
  
  scale_x_datetime("Date", date_labels = "%b %d %I %p")+
  
  #add axes
  scale_y_continuous(
    # Features of the first axis
    name = "NO (ppb)",
    # Add a second axis and specify its features
    sec.axis = sec_axis(~.*100, name="BC (nanograms / m3)")
  )
```

We want to print this plot with plotly, which will enable you to scroll around on the graph and otherwise dynamically interact with it. We do that by passing the ggplot object into the plotly function, like so: 

```{r}
#ggplotly(no_aeth)
```

The datasets look to be about a minute apart, but not in a consistent way. 

## Plotting CPC and BC 

We can use the same process from the last section to plot any variables. In order to change it, we just change the variables we are plotting and the scaling. 

```{r}
cpc_aeth <- ggplot() + 
  #plot cpc data
  geom_line(data=cpc_upload2, aes(x=date, y=concent), color=cbPalette[1]) + 
  #plot BC data
  geom_line(data=aeth_data, aes(x=date, y=100*`Blue BC1`), color=cbPalette[2]) +
  geom_line(data=aeth_data, aes(x=date, y=100*`Red BC1`), color=cbPalette[3]) +
  geom_line(data=aeth_data, aes(x=date, y=100*`Green BC1`), color=cbPalette[4]) +
  geom_line(data=aeth_data, aes(x=date, y=100*`IR BC1`), color=cbPalette[5]) +
  
  #add axes
  scale_y_continuous(
    # Features of the first axis
    name = "CPC concentration",
    # Add a second axis and specify its features
    sec.axis = sec_axis(~.*100, name="BC")
  )


ggplotly(cpc_aeth)
```

# Time averaging 

Next we're going to average cpc to be in 1 minute intervals, and see how well it still aligns with the rest of the data. In this section, we will average the cpc data in the way we've done for the HEPA analysis. I actually took that function and made it into a helper function in a separate file, which I'm just going to call here like a normal function. 

```{r}
#average cpc to 1 minute
time_averaged_cpc <- average_cpc(cpc_upload2)
```

# Merging 

Now that the cpc data is time averaged, we can sync it with the rest of the data. 

First, we round the PM data to the nearest minute: 

```{r}
#round pm to the nearest minute 
aeth_data$original_date <- aeth_data$date
aeth_data$date <- round_date(ymd_hms(aeth_data$date, tz="America/New_York"), unit="minute") #round date for merging
aeth_data<- aeth_data[order(aeth_data$original_date),] #put it in chronological order
```

Then, we merge the dataframes, two at a time: 

```{r}
aeth_cpc <- merge(aeth_data, time_averaged_cpc, by.x = "date", by.y = "date1min", all = FALSE, suffixes = c(".BC",".cpc"))
merged_df <- merge(aeth_cpc, sn45_subset, by = "date", all = FALSE)
```

## Plot Merged Dataframe 

To see what the time averaged CPC looks like, we can plot the merged dataframe using the same methods as above. 

```{r}
cpc_aeth <- ggplot() + 
  #plot cpc data
  geom_line(data=merged_df, aes(x=date, y=concent_1), color=cbPalette[1]) + 
  #plot BC data
  geom_line(data=merged_df, aes(x=date, y=50*`Blue BC1`), color=cbPalette[2]) +
  geom_line(data=merged_df, aes(x=date, y=50*`Red BC1`), color=cbPalette[3]) +
  geom_line(data=merged_df, aes(x=date, y=50*`Green BC1`), color=cbPalette[4]) +
  geom_line(data=merged_df, aes(x=date, y=50*`IR BC1`), color=cbPalette[5]) +
  
  #add axes
  scale_y_continuous(
    # Features of the first axis
    name = "CPC concentration",
    # Add a second axis and specify its features
    sec.axis = sec_axis(~.*50, name="BC")
  )


ggplotly(cpc_aeth)
```


## Subsetting to one period 

We're going to take a subset of all the dataframes, so that we can isolate the peaks for further analysis. 

From visual inspection of the graphs, I think that there is good data during the latter half of the day on October 15. We will subset the data for this timeperiod. 





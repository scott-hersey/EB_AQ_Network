---
title: "Integrating CPC Data"
---



# libraries that we'll need 

```{r warning=FALSE, message = FALSE}
library(data.table)
library(lubridate)
library(tidyr)
library(dplyr)
library(openair)
library(baseline)
```

# importing data (not including CPC)

1. Import Aethalometer data 

UV BC1, Blue BC1, Green BC1, Red BC1, and IR BC1

```{r}
aeth_data <- fread("data/cpc/Data from Week 1/AETHLOMETER/MA350-0093_S0024_200923112201.CSV")

# removing unnecessary variables 

aeth_data <- aeth_data[ , c( "Date / time local",  "Timezone offset (mins)" , "Date local (yyyy/MM/dd)", "Time local (hh:mm:ss)", "Status" ,  "UV BC1", "Blue BC1", "Green BC1", "Red BC1", "IR BC1")]

# formatting date time 

aeth_data$date <- ymd_hms(aeth_data$`Date / time local`)
tz(aeth_data$date) <- "America/New_York"


```


# Concatenate CPC data 

2.	Concatenate the CPC data. Each day is a new file, so you'll need to combine them all into a single vector.

```{r}
nm <- list.files(path="data/cpc/Data from Week 1/MCPC data/", all.files = TRUE, full.names = TRUE, pattern="*.TXT")

cpc_upload <- do.call(rbind, lapply(nm, function(x) read.delim(file=x, header = TRUE, skip = 13) ) )

# removing unncessary variables 

cpc_upload <- cpc_upload[ , c("X.YY.MM.DD", "HR.MN.SC", "concent", "fillcnt"  )]

#formatting datetime

cpc_upload$date <- with(cpc_upload, ymd(`X.YY.MM.DD`) + hms(`HR.MN.SC`))

hour(cpc_upload$date) <- hour(cpc_upload$date) + 8 

# removing NA rows 

final <- cpc_upload[!(any(is.na(cpc_upload))),]
 

```


For the CPC, the vector your care about is "concent." The other one to pay attention to is "fillcnt," which is an error warning that is nonzero if the working fluid ran dry


## Removing error 

```{r}

cpc_upload <- final 

cpc_upload$binary_fillcnt <- ifelse(
    ( 
        (cpc_upload$fillcnt != 0 )
    ),
    1,  # if condition is met, put 1
    0   # else put 0
)

start_indx <- which(diff(c(0L, cpc_upload$binary_fillcnt)) == 1L)
#[1] 1 5 7
end_indx <- which(diff(c(cpc_upload$binary_fillcnt, 0L)) == -1L)

filtering_start_indx <- start_indx - 60*5
filtering_end_indx <- end_indx + 60*5

filtering <- data.frame(filtering_start_indx,filtering_end_indx   )
#filtering <- subset(filtering, filtering_start_indx>1 & filtering_end_indx<length(cpc_upload$fillcnt)+1)

#filtering ones that come before 1
if(any(filtering$filtering_start_indx < 1)){
  #print(which(filtering$filtering_start_indx <1))
  filtering$filtering_start_indx <- replace(filtering$filtering_start_indx, which(filtering$filtering_start_indx <1), 1 )
}


#filtering pairs that start after end 
if(any(filtering$filtering_start_indx > length(cpc_upload$fillcnt))){
  #print(which(filtering$filtering_start_indx <1))
  indxs_pairs <- which(filtering$filtering_start_indx > length(cpc_upload$fillcnt))
  filtering <- filtering[!indxs_pairs]
}



#filtering pairs that end after end 
if(any(filtering$filtering_end_indx > length(cpc_upload$fillcnt))){
  #print(which(filtering$filtering_end_indx > length(cpc_upload$fillcnt)))
  filtering$filtering_end_indx <-replace(filtering$filtering_end_indx, which(filtering$filtering_end_indx > length(cpc_upload$fillcnt)), length(cpc_upload$fillcnt) )
}
setDT(filtering)
filtering <- filtering[!duplicated(filtering$filtering_end_indx)]


remove_indxs <- unlist(Map(':',filtering$filtering_start_indx, filtering$filtering_end_indx))
#cpc_upload <- cpc_upload[-c(remove_indxs), ] 

remove_indxs <- unique(remove_indxs)

cpc_upload[remove_indxs, c("concent", "fillcnt" )] <- NA 
#cpc_upload[remove_indxs] <- NA 
```


## Previous Methods 

```{r} 
# cpc_upload <- na.omit(cpc_upload) 
# 
# length(which(cpc_upload$fillcnt != 0))
# 
# indxs <- which(cpc_upload$fillcnt != 0)
```

```{r}
# list_indxs = vector()
# min5 = 1*60*5;
# 
# for(indx in (2:length(indxs))){
#   if(indxs[indx] -1 != indxs[indx-1] &&  !(indxs[indx-1] %in% list_indxs)){
#     list_indxs <- append(list_indxs, indxs[indx-1])
#     list_indxs <- append(list_indxs, indxs[indx])
#     if(indxs[indx-1] == 343){
#       
#     }
#     else{
#     # remove 5 minutes past the indx where indx -1 stops 
#       
#     # remove 5 minutes before 
#     }
# 
#   }  
# }
```


#### Removing fillcnt data - method 2 

```{r}
# cpc_upload$remove <- rep(0,length(cpc_upload$fillcnt))
# 
# indx = 1
# while(indx != length(cpc_upload$fillcnt)){
#   if(cpc_upload$fillcnt[indx] != 0){
#     indx = indx +1
#   }
#   else{
#       if(cpc_upload$fillcnt[indx+300] != 0){
#         cpc_upload$remove[indx:indx+300] <- 1
#         indx = indx + 301
#   }
#   }
# 
# }


```


# Check for anything funky 

1.2. identify anything funky like negative values. You know the drill.

Gutcheck: 
-# of negative values 
-timeplots (for obvious trends)
- summary stats 

```{r}
timePlot(aeth_data, pollutant = c("UV BC1", "Blue BC1", "Green BC1", "Red BC1", "IR BC1"))
```

-Showing spikes at the same time 
-Has baseline with isolated peak events 
-Looks indicative of individual plume events! 
-I see some negative values on Sept 29

```{r}
timePlot(cpc_upload, pollutant = "concent")
```
- No negative values 
- Quiet period on Sept 29 
- High spike on Sept 23? 

```{r}
aeth_data<- na.omit(aeth_data)
for(pollutant in list(aeth_data$`UV BC1`, aeth_data$`Blue BC1`, aeth_data$`Green BC1`, aeth_data$`Red BC1`, aeth_data$`IR BC1`)){
  print(sum(pollutant < 0))
}
```
So there are a number of negative values. It's about an hour to two hours of negative data for each variable 

```{r}
#cpc_upload <- na.omit(cpc_upload)
sum(cpc_upload$concent < 0)
```
No negative values for this.


# importing sn45 and sn46 

3.	Download data from SN 045 and 046 from the same time period (23-28 Sep).

```{r}
sn45_cpc <- fread("data/cpc/Data from Week 1/sn45cpc.csv", header = TRUE)
sn45_cpc$date <- ymd_hms(sn45_cpc$timestamp, tz="UTC")
sn45_cpc$date <- with_tz(sn45_cpc$date, "America/New_York")
sn46_cpc <- fread("data/cpc/Data from Week 1/sn45cpc.csv", header = TRUE)
sn46_cpc$date <- ymd_hms(sn46_cpc$timestamp, tz="UTC")
sn46_cpc$date <- with_tz(sn46_cpc$date, "America/New_York")
```
Correcting no and no2 

```{r}
no_ae_filter <- function(sensor){
  # NO AE Filter 
  if(any(sensor$no > -3000)){
    sensor[which(sensor$no > -3000), "no"] <- NA
  }


  #create a vector that shows the derivatives 
  no_ae_derivative <- c(0,diff(sensor$no_ae, na.rm = TRUE))
  #creating logical vector to pick which things to get rid of 
  logical_vec <- abs(no_ae_derivative) < abs(2.5*(sd(no_ae_derivative, na.rm=TRUE)))
  
  #make sure SN NO values that == 0 are not removed
  if (any(sensor$no == 0, na.rm = TRUE)){
    sensor$no <- replace(sensor$no, 0, 0.00001)
  }
  # if logical_vec is true, then set the NO value to NA 
  sensor$no <- sensor$no * logical_vec
  sensor$no <- replace(sensor$no, 0, NA)
  
  return(sensor)
}
```

```{r}
no_baseline_filter <- function(sensor){
  # NO BASELINE CORRECTION 
  if(all(is.na(sensor$no))){
    print("no NO data")
    return()
  }
  
  
  tz(sensor$date) <- "America/New_York"
  # create day column
  sensor[, day := as.Date(date, format="%Y-%m-%d", tz = "America/New_York")]
  # create corrected column 
  sensor[, correctedNO := seq(0,0,length.out= length(sensor$no))]
  sensor$correctedNO[sensor$correctedNO == 0] <- NA  #set them actually to NA
  
  dropNAsensor<- sensor[!is.na(sensor$no), ] # drop NO NAs
  
  unique_days <- c(unique(dropNAsensor$day, na.rm=TRUE)) #get all of the unique days in the sensor
  
  for (i in 2:(length(unique_days)-1)){ #for all days
    temp <- subset(dropNAsensor, day %in% unique_days[i], c("day", "no", "date")) #create temp dataset
    
    if (nrow(temp) > 450){
      wholebase.peakDetection <- baseline(t(temp$no), method='peakDetection',left=50, right=50, lwin=10, rwin=10) #baseline correction
      
      #replace the correctedNO column values with the baseline correction from earlier
      dropNAsensor$correctedNO[which(dropNAsensor$date == temp$date[1]): which(dropNAsensor$date == tail(temp$date, n=1))] <-    
        c(getCorrected(wholebase.peakDetection))
    }
    
    else{
      if ( (sum(temp$no < 0, na.rm = TRUE) / nrow(temp)) < 0.25){
        dropNAsensor$correctedNO[which(dropNAsensor$date == temp$date[1]): which(dropNAsensor$date == tail(temp$date, n=1))] <-    
          sensor$no[which(sensor$date == temp$date[1]): which(sensor$date == tail(temp$date, n=1))]
      }
      
    }
    
  }
  
  sensor$correctedNO[which(sensor$date %in% dropNAsensor$date)] <- dropNAsensor$correctedNO
  return(sensor$correctedNO)
}
```




```{r}
sn45_cpc <- no_ae_filter(sn45_cpc)
sn46_cpc <- no_ae_filter(sn46_cpc)
sn45_cpc$correctedNO <- no_baseline_filter(sn45_cpc)
sn46_cpc$correctedNO <- no_baseline_filter(sn46_cpc)
```


Last time, we created an NO2 model variable, based on NO2 model code that Eben had. Since we don't have one this time, I will not include it. 


# Time Sync

4.	Time sync - The CPC is at 1 s and the Aethalometer is 1 min. QuantAQ is 1 min. Probably best to time sync to the same 1 min time basis as the QuantAQ nodes. All of them are on exactly the same time stamp, because I time synced all of the instruments when I put them in the field on the 23rd. 

```{r}
time_sync <- function(sensor){
  sensor <- mutate(sensor, originaldate = date) #keeping original times for comparing to flight data
  sensor$date <- round_date(ymd_hms(sensor$date, tz="America/New_York"), unit="minute") #round date for merging
  sensor<- sensor[order(sensor$originaldate),] #put it in chronological order
  setDT(sensor, key = "date") #change object type to data.table
  sensor <- unique(sensor, by = "date") #remove duplicates
  
  return(sensor)

}
```

```{r}
#sn45_cpcS <- 

sn45_cpcS <- sn45_cpcS %>%
  #time_sync(sn45_cpc) %>%
  select("date", everything())
sn45_cpc <- sn45_cpc[order(sn45_cpc$date),]
sn45_cpc <- sn45_cpc %>% select("date", everything())

sn46_cpc <- time_sync(sn46_cpc)

aeth_data <- time_sync(aeth_data)
```


```{r}
cpc_timesync <- cpc_upload[, c("concent", "fillcnt","date")]

cpc_timesync$date1min <- round_date(cpc_timesync$date, "1 min")
cpc_timesync_1min <- aggregate(cpc_timesync[, names(cpc_timesync) != c("date1min", "date")], by = list(cpc_timesync$date1min), FUN = mean, na.rm = TRUE)
```


```{r}
# sn45_sn46 <- inner_join(sn45_cpc, sn46_cpc, by = "date")
# sn_cpc <- inner_join(sn45_sn46, cpc_timesync_1min, by.x = "date", by.y = "Group.1")
# full_dtst <- inner_join(sn45_sn46, aeth_data, by = "date")

```

```{r}
setDT(sn45_cpc, key = "date") # change to data.table object
setDT(sn46_cpc, key = "date")
setDT(cpc_timesync_1min, key = "Group.1")
setDT(aeth_data, key = "date")
#no2model <- unique(no2model, by = "date") #remove duplicates


sn45_sn46 <- sn45_cpc[sn46_cpc, nomatch = 0] #first join
sn_cpc <- cpc_timesync_1min[sn45_sn46, nomatch = 0] #second join
full_dtst <- sn_cpc[aeth_data, nomatch = 0]
```


```{r}
# any(intersect(sn45_cpc$date, cpc_timesync_1min$Group.1))
# any(intersect(sn46_cpc$date, cpc_timesync_1min$Group.1))
# any(intersect(aeth_data$date, cpc_timesync_1min$Group.1))
# length(intersect(sn_aeth$date, cpc_timesync_1min$Group.1))
# length(intersect(sn45_sn46$date, aeth_data$date))
# length(intersect(sn45_sn46$date, cpc_timesync_1min$Group.1))
# 
# 
# 
# Reduce(intersect, list(sn45_cpc$date,sn46_cpc$date,cpc_timesync_1min$Group.1 ))
```
```{r}
length(unique(cpc_timesync_1min$Group.1))

length(unique(sn45_sn46$date))
```


# Compare 

```{r}
timePlot(full_dtst, pollutant = c("concent","UV BC1", "Blue BC1", "Green BC1", "Red BC1", "IR BC1",
                                "no2",  "i.no2", "no", "i.no","co",  "i.co",  "co2",
                               "i.co2"), y.relation = "free", main = "SN 45 SN 46 Aeth Week 1")
# "correctedNO", "i.correctedNO"
```


```{r}
timePlot(sn45_cpc, pollutant = "correctedNO")
timePlot(sn45_cpc, pollutant = "no")
```



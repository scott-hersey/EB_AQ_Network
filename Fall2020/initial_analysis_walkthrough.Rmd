---
title: "Initial Analysis Walkthrough"
output: html_notebook
---

# Overview

This is a walkthrough of the Eastie Data Science code created during summer and fall of 2020. It includes downloading, cleaning, and doing a high level analysis of the data. At the end, we will export the cleaned datasets. These can be used as inputs to the following two walkthroughs: regime definitions and co-location. 

# Downloading and Importing

We're going to be using two datasets: QuantAQ pollutant concentration data from the [QuantAQ website](https://www.quant-aq.com/) and meteorology data from the [Iowa Environmental Mesonet](https://mesonet.agron.iastate.edu/request/download.phtml?network=MA_ASOS).

## QuantAQ Data 

Download the following QuantAQ files: 

* SN45: final and raw data taken from 7/4/2019 to 10/23/2020
* SN49: final and raw data taken from 7/4/2019 to 10/23/2020
* SN62: final and raw data taken from 7/4/2019 to 10/23/2020
* SN67: final and raw data taken from 7/4/2019 to 10/23/2020
* SN72: final and raw data taken from 7/4/2019 to 10/23/2020

## Meteorology Data 

Select the following station: 
* [BOS] BOSTON/LOGAN INTL

Select the following variables: 

* Wind Direction
* Wind Speed [mph]

Select the date range: 
* 7/4/2019 to 10/24/2020 (this dataset is not inclusive of the last date)

Select this timezone: 
* America/New_York 


Use the following download options:
* csv 
* no latitude/ longitude vectors 
* represent missing data with blank string
* denote trace with blank string

## Storing the data 

In order to easily import the data, we'll store the data in the following way: 

* Store all the QuantAQ data in the folder called "quant_aq", which is a subfolder in "/data" folder of this workspace. When saving, make sure that the file name starts with the sensor name (ie sn45) and contains either "raw" or "final" in the name, to indicate which type of data was imported 
* Store the meteorology data in the "/data" folder in this workspace. Store the file as "metdata.csv"

## Importing Data

### Importing Necessary Libraries 

Our import and analysis relies on functions from a few packages. The following functions must be imported for the code to run. You might get a message that certain functions are "masked" or "overwritten". This means that you imported libraries that have functions with the same name. If you know that these functions do different things from package to package, and want to use a specific one, then you can specific the function's package with package::function. For example, openair::polarPlot()

```{r, warning= FALSE}
library(lubridate) #date and time functions 
library(data.table) #to use the data.table variable type
library(dplyr) #this library allows you to use the %>% operator
library(tidyr) #this library lets you use the complete function to account for time syncing
library(openair) #for plotting and analysis
```



### Import QuantAQ data 

If we want to import only certain variables, we can define what we want to import ahead of time

```{r}
#defining which variables we'll keep
final_vars <- c( "timestamp","timestamp_local",  "temp_manifold", 
                 "co", "no" , "no2", "o3" ,  "pm1" , "pm25","pm10", "co2" ) #final variables to keep 
raw_vars <- c("bin0", "no_ae") #raw variables to keep
```

Next, we'll import all the final and raw datafiles separately. We do this by finding all the csv's with "final" or "raw" in the name, and then applying the R read function to them. If you prefer to import all variables, comment the "sapply" functions that are being used, and uncomment the ones below it. The difference between them is that the currently uncommented one uses the "select" parameter, and the commented one doesn't.

```{r}
#finding all final datasets 
temp = list.files(path = "./data/quant_aq/", pattern = "^.*_final.*\\.csv", full.names = TRUE)
finalfiles = sapply(temp,  FUN = function(x) fread(file=x,  data.table = TRUE, select = final_vars), simplify = FALSE,USE.NAMES = TRUE)
#finalfiles = sapply(temp, function(x) fread(file=x , data.table = TRUE) , simplify = FALSE,USE.NAMES = TRUE)

#finding all raw datasets
temp = list.files(path = "./data/quant_aq", pattern = "^.*_raw.*\\.csv", full.names = TRUE)
rawfiles = sapply(temp, function(x) fread(file=x, data.table = TRUE, select = raw_vars) , simplify = FALSE,USE.NAMES = TRUE) 
#rawfiles = sapply(temp, function(x) fread(file=x,  data.table = TRUE) , simplify = FALSE,USE.NAMES = TRUE)
```

The output of the above functions gives a list of data tables. Since we included the sapply parameter "use.names", it attached the file name to each data.table. Since we used data.table = TRUE, we can be sure that the data is given in the data.table format, which has a more efficient runtime for large datasets.

Next, we want to combine the final and raw files for a given sensor. However, it will be beneficial to keep the sensors as a list of dataframes, as we'll see later.

```{r}
snfiles <- mapply(FUN = cbind,finalfiles,rawfiles,SIMPLIFY = FALSE) # column bind the final and raw dataframes, so that the first final dataframe gets binded to the first raw dataframe, and so on

```

As stated above, the file name is given as the title for each dataset.
```{r}
names(snfiles) # print the current names of the dataframes in the list
```

Instead, we're going to an easier naming convention. Since we used a specific naming scheme and a specific path, we can just extract the part of the path name that describes the dataset. Specifically, we're extracting "sn" and the sensor number. For example, "sn45" or "sn46".

```{r}
new_names <- lapply(names(snfiles), function(x) substr(x, 17, 20)) #extract the real sensor names from the current names used in the list
names(snfiles) <- new_names #rename list names
```




### Import Meteorology data

First, we import the file. We can indicate that there is a header. 

```{r}
metdata <- fread("data/metdata.csv", header=TRUE, data.table = TRUE) #import meteorology file
```

If you look at the file, you'll see that the datetime is not in the standard datetime format. We can standardize it to the quantAQ data format using the R built-in POSIXct function, which changes datetime formats. We can specific which timezone this data is already in, as well. 

```{r}
metdata$date <- as.POSIXct(metdata$valid, format = "%Y-%m-%d %H:%M", tz = "America/New_York") #setting datetime, using correct timezone
```

Next, if we look at data columns, we see that the column names are unrepresentative of the data. if we look at the date column in particular, we see that the data is not in one-minute intervals, like the sensor data, but in 5 minute intervals. To solve this second concern, we can add one-minute intervals in between and then fill those 1 minute intervals with the data from the 5 minute intervals. Let's say that wind direction is 0 for 11:00 and 120 for 11:05. Then, the new data would be: 11:00 - 0, 11:01 - 0, 11:02 - 0, 11:03 - 0, 11:04 - 0, 11:05 - 120, 11:06 - 120 ... 11:09 - 120

We can solve both of these concerns at once, using the dpylr operator (%>%)

```{r}
metdata <- metdata %>%
  setnames(old = c("drct", "sped", "valid"), new = c("wd", "ws", "original_met_time")) %>% #rename
  na.omit("date") %>% #if any dates are NA, the following function won't work
  complete(date = seq(from = min(date), to= max(date),  by = "1 min")) %>%#make 1 minute intervals
  fill(c("wd", "ws")) #fill those new 1 minute interval rows 
```

We specified on the Iowa Mesonet website that our data's null values would show up as a blank string. When we import into R, this will then translate as an NA value. Since the missing strings are NAs, wind speed and wind direction become numeric vectors. If the missing data was set to "null", for example, then the vectors would be strings

```{r}
class(metdata$ws)
class(metdata$wd)
```

On the website, the wind speed was given in in mph. However, we would prefer to use m/s. Let's convert ws from mph to m/s.

```{r}
metdata$ws <- metdata$ws * (1609/3600) #converting to m/s, 1609 meters per mile, 3600 seconds per hr
```

Lastly, let's remove variables that we won't use:

```{r}
metdata[,c( "station")] <- list(NULL) #getting rid of unnecessary variables
```

# Formatting and Merging Data 

## Formatting QuantAQ sensor data 


Similar to the meteorology data, we can identify what needs to be reformatted in the sensor data. 

Let's walk through it step-by-step for SN45. 

### Formatting Walkthrough for SN45 

First, let's standardize the date. Lubridate's ymd_hms function puts datetime data in the format Y:M:D H:M:S, so we'll use it. This function also has a parameter where you can define the timezone. We know the timestamp data is UTC, so we use that. Afterwards, we can move the datetime vector to be in the EST/EDT timezone with the lubridate timezone (tz) function. We're using timestamp instead of timestamp local, because QuantAQ said it was more reliable and because we can check the formatted datetime against the local timestamp afterwards.

```{r}
snfiles$sn45$date <- lubridate::ymd_hms(snfiles$sn45$timestamp, tz="UTC") #parse datetime
snfiles$sn45$date <- lubridate::with_tz(snfiles$sn45$date, "America/New_York") #change timezone
```

Next, we want to round the data to the nearest minute, so that we could align it with the meteorological dataset. We'll preserve the original datetime vector by saving it as a separate vector. Then, we'll round the data so 11:30:29 rounds to 11:30:00 and 11:30:31 rounds to 11:31:00. 

```{r}
snfiles$sn45 <- mutate(snfiles$sn45, originaldate = date) #keeping original times for comparing to flight data
snfiles$sn45$date <- round_date(snfiles$sn45$date, unit="minute") #round date for merging
```

Lastly, we'll reorder the data so it's chronological, instead of reverse chronological. 

```{r}
snfiles$sn45 <- snfiles$sn45[order(snfiles$sn45$originaldate),] #put it in chronological order
```


This data has duplicate rows, which can become a problem for some analyses.

```{r}
sum(duplicated(snfiles$sn45) == 1) #checking the number of duplicate entries 
```

Let's see what they look like: 

```{r}
snfiles$sn45[duplicated(snfiles$sn45) | duplicated(snfiles$sn45, fromLast=TRUE)] #print all duplicates, including the first one
```

These rows are entries that are exactly the same (all the columns are the same, even timestamp). Since there is no difference between this information, we can remove the duplicates: 

```{r}
snfiles$sn45 <- unique(snfiles$sn45 ) #keep only unique rows in the dataset
```



### Formatting the rest of the QuantAQ files 

We'll do what we did for sn45 for all of the QuantAQ files. 

In order to easily apply the cleaning to all the datasets, we're going to create a function to apply to the list of data frames. Below is the function.

```{r}
clean_sensor_data <- function(sensor){
  sensor$date <- ymd_hms(sensor$timestamp, tz="UTC") #parse datetime
  sensor$date <- with_tz(sensor$date, "America/New_York") #change the time according to shifted timezone
  sensor <- mutate(sensor, originaldate = date) #keeping original times for comparing to flight data
  sensor$date <- round_date(ymd_hms(sensor$date, tz="America/New_York"), unit="minute") #round date for merging
  sensor<- sensor[order(sensor$originaldate),] #put it in chronological order
  sensor <- unique(sensor) #removing duplicate rows
  return(sensor)
}
```

In order to apply it to every dataframe, we can run lapply (list apply) which will apply a certain function to every element in a list. We're going to run it on every element of the snfiles list, except the first (sn45), since we did that above.

```{r}
snfiles[c(-1)] <- lapply(snfiles[c(-1)], function(x) clean_sensor_data(x)) 
```


## Merging 

From trial and error, I've found that joining datasets using DT's merge works the best. Below is the code to implement it. First, we make sure that each dataset is a data.table with a key column of "date". This will allow us to the data.table merge function. We merge the meteorological and sensor datasets by date with no_match = 0. no_match = 0 ensures that there's no duplicates. 

First, we'll implement it for SN45.

```{r}
setDT(metdata, key = "date") # make the meteorological data a data.table
setkey(snfiles$sn45,  "date") #set the "key" column to date

snfiles$sn45 <- snfiles$sn45[metdata, nomatch = 0] #merge
```

Now, let's implement it for the rest of the data frames. Again, we'll create a merge function and apply it to every element of the snfiles object (in other words, to every dataset).

```{r}
merge_metdata <- function(sensor, metdata){
  setkey(sensor, "date")
  sensor <- sensor[metdata, nomatch = 0]
  return(sensor)
}
```

```{r}
snfiles[c(-1)] <- lapply(snfiles[c(-1)], function(x) merge_metdata(x, metdata))
```
Again, we exclude the first, since we already merged it above.

# Cleaning

There are a number of erroneous datapoints, or data quirks, which come inherently in from sensors. In this section, we'll implement a number of filters.

## O3 filter 

According to QuantAQ, all o3 negative o3 data is erroneous. However there is no trend to the negative o3 data, so we'll set all negative o3 data to 0. This is a simple process, so we'll just apply it directly to the datasets with lapply. 


```{r}
o3_filter <- function(sensor){ #create o3 filter function
  sensor$o3 <- replace(sensor$o3, sensor$o3 < 0, 0)
  return(sensor)
}

snfiles<- lapply(snfiles, function(x) o3_filter(x)) #apply it to the data frames
```


## NO_ae filter 

The NO auxiliary electrode (no_ae) is an NO sensor which should be very stable. If there is a drastic change in no_ae, the data is likely wrong. We want to exclude these data points that are likely wrong, therefore, we'll set NO to NA when no_ae changes rapidly. We'll set NO to NA when the change in no_ae exceeds 2.5 standard deviations past its mean. 

```{r}
no_ae_filter <- function(sensor){
  #create a vector that shows the derivatives 
  no_ae_derivative <- c(0,diff(sensor$no_ae, na.rm = TRUE))
  #creating logical vector to pick which things to get rid of 
  logical_vec <- abs(no_ae_derivative) < abs(2.5*(sd(no_ae_derivative, na.rm=TRUE)))
  
  #make sure SN NO values that == 0 are not removed
  if (any(sensor$no == 0, na.rm = TRUE)){
    sensor$no <- replace(sensor$no, 0, 0.00001)
  }
  # if logical_vec is true, then set the NO value to NA 
  sensor$no <- sensor$no * logical_vec
  sensor$no <- replace(sensor$no, 0, NA)
  
  return(sensor)
}
  
```

```{r}
snfiles<- lapply(snfiles, function(x) no_ae_filter(x))
```


## NO baseline correction 

With the new machine learning algorithm that QuantAQ applies to their sensor data, the baseline for the data is already corrected. Below, I'm including the code written for No baseline correction, that was used before the machine learning algorithm was developed. The following code will be commented, because it should not be run with the new data. 

Note, that if you want to run this section, you'll need to define "sensor". For example, "sensor <- snfiles$sn45".


The first step is creating a column that represents the day that each data point was taken on. 

```{r}
# # create day column
# sensor[, day := as.Date(date, format="%Y-%m-%d", tz = "America/New_York")]
```

Next, we'll create a column that will be filled with baseline corrected data: 

```{r}
# # create corrected column 
# sensor[, correctedNO := seq(0,0,length.out= length(sensor$no))]
# sensor$correctedNO[sensor$correctedNO == 0] <- NA  #set them actually to NA
```

We need to drop all NA values in order for the baseline algorithm to work. We do that here: 

```{r}
# dropNAsensor<- sensor[!is.na(sensor$no), ] # drop NO NAs
```

We'll create a list of all the unique days in the dataset, to loop through: 

```{r}
# unique_days <- c(unique(dropNAsensor$day, na.rm=TRUE)) #get all of the unique days in the sensor
```

We're going to baseline-correct the NO day by day. So, we'll loop through all the unique days in the data (given by the code above), and apply the baseline correcting algorithm to all the data with the date given by the for loop. 

If there is not enough data points in the day, then the function will not run. Therefore, if we determine there's too few datapoints (less then 550), and the data is mostly positive, we include the original NO data. Otherwise, we'll keep the corrected data as NA.

```{r}
# for (i in 2:(length(unique_days)-1)){ #for all days
#   temp <- subset(dropNAsensor, day %in% unique_days[i], c("day", "no", "date")) #create temp dataset
# 
#   if (nrow(temp) > 550){
#     wholebase.peakDetection <- baseline(t(temp$no), method='peakDetection',left=50, right=50, lwin=10, rwin=10) #baseline correction
#   
#   #replace the correctedNO column values with the baseline correction from earlier
#   dropNAsensor$correctedNO[which(dropNAsensor$date == temp$date[1]): which(dropNAsensor$date == tail(temp$date, n=1))] <-    c(getCorrected(wholebase.peakDetection))
#   }
#   
#   else{
#     if (sum(temp$no < 0, na.rm = TRUE) / nrow(temp) < 0.25){
#       dropNAsensor$correctedNO[which(dropNAsensor$date == temp$date[1]): which(dropNAsensor$date == tail(temp$date, n=1))] <-    
#         sensor$no[which(sensor$date == temp$date[1]): which(sensor$date == tail(temp$date, n=1))]
#     }
# 
#   }
#   
# }
# 
# sensor$correctedNO[which(sensor$date %in% dropNAsensor$date)] <- dropNAsensor$correctedNO # replace values based on date 
```



# Analysis 

## Polar Plots 

Polar plots are a tool that shows the relationship between pollutant concentration and wind direction and speed. In this section, we'll generate polar plots for NO, NO2, CO, CO2, bin0 and PM1. We'll do this by creating a function that will create all these polar plots, for a given sensor dataset. We'll apply this function to the snfiles list. 

```{r}
generate_overall_polarplots <- function(sn, snstring){
  #generates the polar plots for the variables we are interested in 
  polarPlot(sn, pollutant = "no", main = paste0(snstring, " NO Polar Plot", sep= " "))
  polarPlot(sn, pollutant = "no2", main = paste0(snstring, " NO2 Polar Plot", sep= " "))
  polarPlot(sn, pollutant = "co",  main = paste0(snstring, " CO Polar Plot", sep= " "))
  polarPlot(sn, pollutant = "bin0",  main = paste0(snstring, " bin0 Polar Plot", sep= " "))
  polarPlot(sn, pollutant = "pm1",  main = paste0(snstring, " PM1 Polar Plot", sep= " "))

}
```


We're applying this function as a for loop to the datasets, so we can also loop through the dataset names, and add those to the titles. 

```{r}
for (i in seq(length(names(snfiles)))){
  generate_overall_polarplots(snfiles[[i]], names(snfiles)[i])
}
```



## Diurnal Plots 

Diurnal plots show the relationship between pollutant concentration and time (on the scale of hours, days and months). To create diurnal profiles for all of the sensors, we can apply the same method as we did to create polar plots.

```{r}

for (i in seq(length(names(snfiles)))){
  
  timeVariation(snfiles[[i]], pollutant = c("no2"), main = paste0(names(snfiles)[i], " NO2 Diurnal Profile"))
  
  timeVariation(snfiles[[i]], pollutant = c("pm1", "co", "no",  "co2", "bin0"), normalise = TRUE, main = paste0(names(snfiles)[i], " Normalized Group of Pollutants Diurnal Profile"))
}
```




# Export 

The last thing we'll do is export the data into csvs. 

Since we want to save the datasets to files with representative names, we can use mapply (like lapply but with more functionality). For each dataset, we can set the name we used when we set snfiles as the file names. 

```{r}
mapply(
  write.table, #apply function write table
  x=snfiles, file=paste(names(snfiles), "csv", sep="."), #for each element in snfiles, use its name to make a csv of it
  MoreArgs=list(row.names=FALSE, sep=",")
)
```

Place the exported files into the data folder.

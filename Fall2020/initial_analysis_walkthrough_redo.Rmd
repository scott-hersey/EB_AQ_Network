---
title: "Initial Analysis Walkthrough"
output: html_notebook
---

# Overview

This is a walkthrough of the Eastie Data Science code created during summer and fall of 2020. It includes downloading, cleaning, and doing a high level analysis of the data. At the end, we will export the cleaned datasets. These can be used as inputs to the following two walkthroughs: regime definitions and co-location. 

# Downloading and Importing

We're going to be using two datasets: QuantAQ pollutant concentration data from the [QuantAQ website](https://www.quant-aq.com/) and meteorology data from the [Iowa Environmental Mesonet](https://mesonet.agron.iastate.edu/request/download.phtml?network=MA_ASOS).

Note: The paragraph above and the following section will describe the downloading, storing and importing process for a normal workflow. However, there is not always the option of using this normal workflow. After the following downloading, storing and importing sections, there will be a section that describes how to do these things when you need to use the QuantAQ data from the [DropBox folder](https://app.box.com/s/3rs3dasqitqtrwxgtem0ef96jhkbnqtm). 

## QuantAQ Data 

Download the following QuantAQ files: 

* SN45: final and raw data taken from 9/7/2019 to 10/27/2020
* SN49: final and raw data taken from 9/7/2019 to 10/27/2020
* SN62: final and raw data taken from 9/7/2019 to 10/27/2020
* SN67: final and raw data taken from 9/7/2019 to 10/27/2020
* SN72: final and raw data taken from 9/7/2019 to 10/27/2020

## Meteorology Data 

Select the following station: 
* [BOS] BOSTON/LOGAN INTL

Select the following variables: 

* Wind Direction
* Wind Speed [mph]

Select the date range: 
* 9/7/2019 to 10/28/2020 (this dataset is not inclusive of the last date)

Select this timezone: 
* America/New_York 


Use the following download options:
* csv 
* no latitude/ longitude vectors 
* represent missing data with blank string
* denote trace with blank string

## Storing the data 

In order to easily import the data, we'll store the data in the following way: 

* Store all the QuantAQ data in the folder called "quant_aq", which is a subfolder in "/data" folder of this workspace. When saving, make sure that the file name starts with the sensor name (ie sn45) and contains either "raw" or "final" in the name, to indicate which type of data was imported. For example: "sn45_final.csv". 
* Store the meteorology data in the "/data" folder in this workspace. Store the file as "metdata.csv"

## Importing Data

### Importing Necessary Libraries 

Our import and analysis relies on functions from a few packages. The following functions must be imported for the code to run. You might get a message that certain functions are "masked" or "overwritten". This means that you imported libraries that have functions with the same name. If you know that these functions do different things from package to package, and want to use a specific one, then you can specific the function's package with package::function. For example, openair::polarPlot()

```{r, warning= FALSE, error = FALSE}
library(lubridate) #date and time functions 
library(data.table) #to use the data.table variable type
library(dplyr) #this library allows you to use the %>% operator
library(tidyr) #this library lets you use the complete function to account for time syncing
library(openair) #for plotting and analysis
library(stringr)
library(baseline) # for baseline correction
```



### Import QuantAQ data 

If we want to import only certain variables, we can define what we want to import ahead of time

```{r}
# #defining which variables we'll keep
# final_vars <- c( "timestamp","timestamp_local",  "temp_manifold",
#                  "co", "no" , "no2", "o3" ,  "pm1" , "pm25","pm10", "co2" ) #final variables to keep
# raw_vars <- c("timestamp","bin0", "bin1", "bin2", "bin3", "bin4", "bin5", "no_ae", "co_ae") #raw variables to keep
```

Next, we'll import all the final and raw datafiles separately. We do this by finding all the csv's with "final" or "raw" in the name, and then applying the R read function to them. If you prefer to import all variables, comment the "sapply" functions that are being used, and uncomment the ones below it. The difference between them is that the currently uncommented one uses the "select" parameter, and the commented one doesn't.

```{r}
# #finding all final datasets
# temp = list.files(path = "./data/quant_aq/", pattern = "^.*_final.*\\.csv", full.names = TRUE)
# finalfiles = sapply(temp,  FUN = function(x) fread(file=x,  data.table = TRUE, select = final_vars), simplify = FALSE,USE.NAMES = TRUE)
# #finalfiles = sapply(temp, function(x) fread(file=x , data.table = TRUE) , simplify = FALSE,USE.NAMES = TRUE)
# 
# #finding all raw datasets
# temp = list.files(path = "./data/quant_aq", pattern = "^.*_raw.*\\.csv", full.names = TRUE)
# rawfiles = sapply(temp, function(x) fread(file=x, data.table = TRUE, select = raw_vars) , simplify = FALSE,USE.NAMES = TRUE)
# #rawfiles = sapply(temp, function(x) fread(file=x,  data.table = TRUE) , simplify = FALSE,USE.NAMES = TRUE)
```

The output of the above functions gives a list of data tables. Since we included the sapply parameter "use.names", it attached the file name to each data.table. Since we used data.table = TRUE, we can be sure that the data is given in the data.table format, which has a more efficient runtime for large datasets.

Let's use an easier naming conventon. Since we used a specific naming scheme and a specific path, we can just extract the part of the path name that describes the dataset. Specifically, we're extracting "sn" and the sensor number. For example, "sn45" or "sn46".

```{r}
# names(finalfiles)<- lapply(names(finalfiles), function(x) substr(x, 17, 20)) #extract the real sensor names from the current names used in the list
# names(rawfiles)<- lapply(names(rawfiles), function(x) substr(x, 17, 20))
# names(finalfiles)
# names(rawfiles)
``` 

Next, we want to combine the final and raw files for a given sensor. However, it will be beneficial to keep the sensors as a list of dataframes, as we'll see later.

Since the final and raw datasets could have a different number of rows, we'll merge them based on entries to the "timestamp" column.


```{r}
# snfiles <- sapply(names(finalfiles), function(k) merge.data.table(finalfiles[[k]], rawfiles[[k]], by="timestamp", all = FALSE), simplify = FALSE,USE.NAMES = TRUE)
```


#### Proof that Combining Datasets works 

In case you aren't following the normal workflow, here is proof that binding the final and raw files still works. 

Let's download SN45 and SN46 data from the QuantAQ website. Download a final and raw datafile, from 10/23 to 10/30. Save these files as "sn45_trial_final.csv" and "..raw.csv", and put them in the "quant_aq" data folder. 

Import those files here.

```{r}
sn45_raw <- fread("./data/quant_aq/sn45_trial_raw.csv", data.table = TRUE) # import final sn45 data
sn45_final <- fread("./data/quant_aq/sn45_trial_final.csv", data.table = TRUE) # import final sn46 data

sn46_raw <- fread("./data/quant_aq/sn46_trial_raw.csv", data.table = TRUE) # import raw sn45 data
sn46_final <- fread("./data/quant_aq/sn46_trial_final.csv", data.table = TRUE) # import raw sn46 data
```

Let's try merging SN45 together based on the "timestamp" vector. 
```{r}
merge(sn45_raw, sn45_final, by = "timestamp", all = FALSE) #merge both sn45 files
```

Now let's simulate the list of final dataframes and raw dataframes. 
```{r}
finalfiles <- list("sn45" = sn45_final,"sn46" = sn46_final) # create list of final dataframes
rawfiles <- list("sn45" = sn45_raw, "sn46" = sn46_raw) # create list of raw dataframes
```

We'll bind the appropriate files together in an lapply statement.

```{r}
snfiles_trial <- sapply(names(finalfiles), function(k) merge.data.table(finalfiles[[k]], rawfiles[[k]], by="timestamp", all = FALSE), simplify = FALSE,USE.NAMES = TRUE) #bind lists of final and raw dataframes
```

This is what's contained in the new dataframes
```{r}
colnames(snfiles_trial$sn45)
```



### Import Meteorology data

First, we import the file. We can indicate that there is a header. 

```{r}
metdata <- fread("data/metdata.csv", header=TRUE, data.table = TRUE) #import meteorology file
```

If you look at the file, you'll see that the datetime is not in the standard datetime format. We can standardize it to the quantAQ data format using the R built-in POSIXct function, which changes datetime formats. We can specific which timezone this data is already in, as well. 

```{r}
metdata$date <- as.POSIXct(metdata$valid, format = "%Y-%m-%d %H:%M", tz = "America/New_York") #setting datetime, using correct timezone
```

Next, if we look at data columns, we see that the column names are unrepresentative of the data. if we look at the date column in particular, we see that the data is not in one-minute intervals, like the sensor data, but in 5 minute intervals. To solve this second concern, we can add one-minute intervals in between and then fill those 1 minute intervals with the data from the 5 minute intervals. Let's say that wind direction is 0 for 11:00 and 120 for 11:05. Then, the new data would be: 11:00 - 0, 11:01 - 0, 11:02 - 0, 11:03 - 0, 11:04 - 0, 11:05 - 120, 11:06 - 120 ... 11:09 - 120

We can solve both of these concerns at once, using the dpylr operator (%>%)

```{r}
metdata <- metdata %>%
  setnames(old = c("drct", "sped", "valid"), new = c("wd", "ws", "original_met_time")) %>% #rename
  na.omit("date") %>% #if any dates are NA, the following function won't work
  complete(date = seq(from = min(date), to= max(date),  by = "1 min")) %>%#make 1 minute intervals
  fill(c("wd", "ws")) #fill those new 1 minute interval rows 
```

We specified on the Iowa Mesonet website that our data's null values would show up as a blank string. When we import into R, this will then translate as an NA value. Since the missing strings are NAs, wind speed and wind direction become numeric vectors. If the missing data was set to "null", for example, then the vectors would be strings

```{r}
class(metdata$ws)
class(metdata$wd)
```

On the website, the wind speed was given in in mph. However, we would prefer to use m/s. Let's convert ws from mph to m/s.

```{r}
metdata$ws <- metdata$ws * (1609/3600) #converting to m/s, 1609 meters per mile, 3600 seconds per hr
```

Lastly, let's remove variables that we won't use:

```{r}
metdata[,c( "station")] <- list(NULL) #getting rid of unnecessary variables
```

# Downloading, Saving and Importing using Dropbox workflow 

## Downloading 

Click "Download" in the top right corner of the Dropbox screen. This will download all of the contents in the DropBox folder. 

## Saving 

The data will download as a zip file. Extract the contents of the zip file, and place them in the "quant_aq" subfolder of the "data" folder in this workspace. Make sure to save the files directly to that folder, and not in a "Final_Customer" subfolder.

## Importing 

These datafiles aren't divided into final and raw, so we don't have to combine files. Instead, we can create one list of variables we want to extract (instead of two, like in the normal workflow). 

```{r}
#defining which variables we'll keep
final_vars <- c( "timestamp","timestamp_local",  "temp_manifold", 
                 "co", "no" , "no2", "o3" ,  "pm1" , "pm25","pm10", "co2" , "no_ae", "bin0", "bin1", "bin2", "bin3", "bin4", "bin5", "bin6") 
```

The data files can be categorized based on which sensor they represent. 

Initially, the data files were categorized by whether they are "f0" or "f1" (both of these categorizes are represented in the filename). The difference between f0 and f1 is that one doesn't have filters apply, and the other does. 

Currently, there are only "f0" files, so we'll specify we're looking at datafiles with "f0" in the name. 

```{r}
temp = list.files(path = "./data/quant_aq/", pattern = "^.*-f0.*\\.csv", full.names = TRUE) #specify files we're looking for
snfiles = sapply(temp,  FUN = function(x) fread(file=x,  data.table = TRUE, select = final_vars), simplify = FALSE,USE.NAMES = TRUE) #importing them
```
Like in the normal workflow, we can reformat the names. Again, we'll only keep the relevant parts of the filenames. 

```{r}
new_names <- lapply(names(snfiles), function(x) substr(x, 17, 26)) #extract the real sensor names from the current names used in the list
names(snfiles) <- new_names #rename list names
names(snfiles)
```

As we see, these are in a different format than the ones imported from the QuantAQ website. We can change this by doing a string replacement - "sn" instead of "Output-0"

```{r}
names(snfiles) <- lapply(names(snfiles), function(x) str_replace(x, "Output-0", "sn"))
names(snfiles)
```



# Formatting and Merging Data 

## Formatting QuantAQ sensor data 


Similar to the meteorology data, we can identify what needs to be reformatted in the sensor data. 

Let's walk through it step-by-step for SN45. 

### Formatting Walkthrough for SN45 

First, let's standardize the date. The ymd_hms function puts datetime data in the format Y:M:D H:M:S, so we'll use it. This function also has a parameter where you can define the timezone. We know the timestamp data is UTC, so we use that. Afterwards, we can move the datetime vector to be in the EST/EDT timezone with the timezone (tz) function. We're using timestamp instead of timestamp local, because QuantAQ said it was more reliable and because we can check the formatted datetime against the local timestamp afterwards.

We'll print a date vector at each step, to show what the functions are doing. 

```{r}

sprintf("unmodified timestamp : %s, timezone : %s", snfiles$sn45$timestamp[1], tz(snfiles$sn45$timestamp[1]))

snfiles$sn45$date <- lubridate::ymd_hms(snfiles$sn45$timestamp, tz="UTC") #parse datetime

sprintf("modified timestamp : %s , timezone : %s", snfiles$sn45$date[1] , tz(snfiles$sn45$timestamp[1]))

snfiles$sn45$date <- lubridate::with_tz(snfiles$sn45$date, "America/New_York") #change timezone

sprintf("modified timezone: %s , timezone : %s ", snfiles$sn45$date[1], tz(snfiles$sn45$date[1]))
sprintf("compared to local timestamp: %s", snfiles$sn45$timestamp_local[1])
```

Next, we want to round the data to the nearest minute, so that we could align it with the meteorological dataset. We'll preserve the original datetime vector by saving it as a separate vector. Then, we'll round the data so 11:30:29 rounds to 11:30:00 and 11:30:31 rounds to 11:31:00. 

```{r}
snfiles$sn45 <- mutate(snfiles$sn45, originaldate = date) #keeping original times for comparing to flight data
sprintf("new date object : %s, old date object : %s", snfiles$sn45$date[1], snfiles$sn45$originaldate[1])
snfiles$sn45$date <- round_date(snfiles$sn45$date, unit="minute") #round date for merging
sprintf("new date object : %s, old date object : %s", snfiles$sn45$date[1], snfiles$sn45$originaldate[1])
```

Lastly, we'll reorder the data so it's chronological, instead of reverse chronological. 

```{r}
sprintf("first date entry : %s", snfiles$sn45$date[1])
snfiles$sn45 <- snfiles$sn45[order(snfiles$sn45$originaldate),] #put it in chronological order
sprintf("first date entry : %s", snfiles$sn45$date[1])
```

### Formatting the rest of the QuantAQ files 

We'll do what we did for sn45 for all of the QuantAQ files. 

In order to easily apply the cleaning to all the datasets, we're going to create a function to apply to the list of data frames. Below is the function.

```{r}
clean_sensor_data <- function(sensor){
  sensor$date <- ymd_hms(sensor$timestamp, tz="UTC") #parse datetime
  sensor$date <- with_tz(sensor$date, "America/New_York") #change the time according to shifted timezone
  sensor <- mutate(sensor, originaldate = date) #keeping original times for comparing to flight data
  sensor$date <- round_date(ymd_hms(sensor$date, tz="America/New_York"), unit="minute") #round date for merging
  sensor<- sensor[order(sensor$originaldate),] #put it in chronological order
  return(sensor)
}
```

In order to apply it to every dataframe, we can run lapply (list apply) which will apply a certain function to every element in a list. We're going to run it on every element of the snfiles list, except the first (sn45), since we did that above.

```{r}
snfiles[c(-1)] <- lapply(snfiles[c(-1)], function(x) clean_sensor_data(x)) 
```

## Merging 

From trial and error, I've found that joining datasets using DT's merge works the best. Below is the code to implement it. First, we make sure that each dataset is a data.table with a key column of "date". This will allow us to the data.table merge function. We merge the meteorological and sensor datasets by date with no_match = 0. no_match = 0 ensures that there's no duplicates. 

First, we'll implement it for SN45.

```{r}
setDT(metdata, key = "date") # make the meteorological data a data.table
setkey(snfiles$sn45,  "date") #set the "key" column to date
sprintf("Original number of sensor data rows: %d", nrow(snfiles$sn45))
sprintf("Original number of met data rows: %d", nrow(metdata))
#sprintf("The number of dates that are the same in the sensor and metdata datetimes: %d", length(intersect(snfiles$sn45$date, metdata$date)))

snfiles$sn45 <- snfiles$sn45[metdata, nomatch = 0] #merge
sprintf("New number of rows in merged data: %d", nrow(snfiles$sn45))
```
Let's see what the vector looks like now: 
```{r}
colnames(snfiles$sn45)
```
As you can see, there is only one date column, all of the sensor columns, wind direction, wind speed, and the original meteorological datetime vector. 

Now, let's implement it for the rest of the data frames. Again, we'll create a merge function and apply it to every element of the snfiles object (in other words, to every dataset).

```{r}
merge_metdata <- function(sensor, metdata){
  setkey(sensor, "date")
  sensor <- sensor[metdata, nomatch = 0]
  return(sensor)
}
```

```{r}
snfiles[c(-1)] <- lapply(snfiles[c(-1)], function(x) merge_metdata(x, metdata))
```
As before, we exclude the first dataframe, since we already merged it above.


# Data Cleaning

## Checking the data

First, we're going to look at statistics of the data, to make sure that there isn't a large problem with the data (ie everything is NA). We're going to do this by creating a function which prints out how many values are negative, zero and NA in the data. Then we're going to use lapply to apply it to all the datasets.


```{r}
data_check_test <- function(sensor){
  
  temp_df <- sensor[, c( "co", "no" , "no2", "o3" ,  "pm1" , "co2", "no_ae", "bin0") ] #define which variables to inspect
  
  na_count <- apply(temp_df, 2,  function(x) sum(is.na(x))) #count the number of NA values
  
  negative_count <-apply(temp_df, 2,  function(x) sum(x<0, na.rm = TRUE)) #count the number of negative values
  
  zero_count <- apply(temp_df, 2,  function(x) sum(x==0, na.rm = TRUE)) #count the number of zero values
  
  return(data.frame(na_count, negative_count, zero_count))
}
```

We'll apply it to our list of dataframes: 

```{r}
lapply(snfiles, function(x) data_check_test(x))
```

We'll also test for the mean, 25th percentile and 75th percentile of each variable. This will tell us if the data is in a reasonable range. 

We'll create two functions: one which computes the percentiles and one which computes the mean.

```{r}
data_check_percentiles <- function(sensor){
  
  temp_df <- sensor[, c( "co", "no" , "no2", "o3" ,  "pm1" , "co2", "no_ae", "bin0") ] #define which variables to inspect
  
  percentiles <-apply(temp_df, 2,  function(x) quantile(x, probs =c(0.25, 0.75), na.rm = TRUE)) # apply the quantile function, which calculates percentiles
  
  return(percentiles)
}
data_check_mean <- function(sensor){
  
  temp_df <- sensor[, c( "co", "no" , "no2", "o3" ,  "pm1" , "co2", "no_ae", "bin0") ] #define which variables to inspect
  
  mean_vals <-apply(temp_df, 2,  function(x) mean(x, na.rm = TRUE)) #apply the mean function
  
  return(mean_vals)
}
```

We'll apply these functions to our datasets:
```{r}
lapply(snfiles, function(x) data_check_percentiles(x))
lapply(snfiles, function(x) data_check_mean(x))
```



## Clean Duplicate Rows 

#### Data by Row 

There are some rows that are exact duplicates. 

```{r}
sum(duplicated(snfiles$sn45) == 1) #checking the number of duplicate entries 
```

Let's see what they look like: 

```{r}
snfiles$sn45[duplicated(snfiles$sn45) | duplicated(snfiles$sn45, fromLast=TRUE)] #print all duplicates, including the first one
```

These rows are entries that are exactly the same (all the columns are the same, even timestamp). Since there is no difference between this information, we can remove the duplicates: 

```{r}
snfiles <- lapply(snfiles, function(x) unique(x)) #keep only unique rows in the dataset
```


We have to remove these first, because they would otherwise skew our pollutant concentration analysis.

Let's make sure they were removed: 

```{r}
snfiles$sn45[duplicated(snfiles$sn45) | duplicated(snfiles$sn45, fromLast=TRUE)] #print all duplicates, including the first one
```


## Removing Negative Values 

First, we'll remove negative values. For this, we are simply finding all the points in all the pollutant vectors which are less than 0, and then setting them to 0. More filters can be added by adding in another replace statement in the same format. 

```{r}
negative_value_filter <- function(sensor){
  sensor$o3 <- replace(sensor$o3, sensor$o3 < 0, 0)
  sensor$co <- replace(sensor$co, sensor$co <0, 0)
  sensor$co2 <- replace(sensor$co2, sensor$co2 <0, 0)
  sensor$no2 <- replace(sensor$no2, sensor$no2 <0, 0)
  sensor$bin0 <- replace(sensor$bin0, sensor$bin0 <0, 0)
  sensor$pm1 <- replace(sensor$pm1, sensor$pm1 <0, 0)
  return(sensor)
}
```

We'll apply this function to all the datasets:

```{r}
#sn45 <- negative_value_filter(sn45)
snfiles <- lapply(snfiles, function(x) negative_value_filter(x))
```

## Removing NO Negative Values 

NO has a changing baseline. It can't be filtered like the ones above because then we would lose real data. Therefore, we're going to apply a baseline correction algorithm to the NO data that was developed for spectroscopy. Since the baseline function wasn't made for this application, we have to prepare the data and feed it into the baseline function incrementally. We'll do this in the no_baseline_filter wrapper function below. The wrapper function was developed during summer 2020, and there is a document describing its components on the shared drive. 

```{r}
no_baseline_filter <- function(sensor){
  # create day column
  sensor[, day := as.Date(date, format="%Y-%m-%d", tz = "America/New_York")]
  
  # create corrected column
  sensor[, correctedNO := seq(0,0,length.out= length(sensor$no))]
  sensor$correctedNO[sensor$correctedNO == 0] <- NA  #set them actually to NA
  
  dropNAsensor<- sensor[!is.na(sensor$no), ] # drop NO NAs
  unique_days <- c(unique(dropNAsensor$day, na.rm=TRUE)) #get all of the unique days in the sensor
  
  for (i in 2:(length(unique_days)-1)){ #for all days
    temp <- subset(dropNAsensor, day %in% unique_days[i], c("day", "no", "date")) #create temp dataset
  
    if (nrow(temp) > 550){
      wholebase.peakDetection <- baseline(t(temp$no), method='peakDetection',left=50, right=50, lwin=10, rwin=10) #baseline correction
  
    #replace the correctedNO column values with the baseline correction from earlier
    dropNAsensor$correctedNO[which(dropNAsensor$date == temp$date[1]): which(dropNAsensor$date == tail(temp$date, n=1))] <-    c(getCorrected(wholebase.peakDetection))
    }
  
    else{
      if (sum(temp$no < 0, na.rm = TRUE) / nrow(temp) < 0.25){
        dropNAsensor$correctedNO[which(dropNAsensor$date == temp$date[1]): which(dropNAsensor$date == tail(temp$date, n=1))] <-
          sensor$no[which(sensor$date == temp$date[1]): which(sensor$date == tail(temp$date, n=1))]
      }
  
    }
  
  }
  
  sensor$correctedNO[which(sensor$date %in% dropNAsensor$date)] <- dropNAsensor$correctedNO # replace values based on date
  
  return(sensor)
}
```

We'll apply this function to all the datasets:

```{r}
#sn45 <- negative_value_filter(sn45)
snfiles <- lapply(snfiles, function(x) no_baseline_filter(x))
```




## Removing High Values without flags

There are unrealistically high values which we know can't be real and we don't have to check. We'll filter those out here. 

```{r}
o3_filter <- function(sensor){ #create o3 filter function
  sensor$o3 <- replace(sensor$o3, sensor$o3 > 300, NA)
  return(sensor)
}
```

```{r}
#sn45 <- o3_filter(sn45)
snfiles <- lapply(snfiles, function(x) o3_filter(x))
```


## Removing High Values with flags 

```{r}
apply_flags <- function(sensor){
  # creating a flags column to flag high values 
  # flag number to pollutant definition given below:
  # co - 1 
  # co2 - 2
  # no2 - 3
  # bin0 - 4
  # pm1 - 5
  # no - 6
  
  sensor$flag <- replicate(nrow(sensor), c(), simplify = FALSE) #create flags column which has a list for every data point
  sensor$flag[which(sensor$co2 > 2000)] <- lapply(sensor$flag[which(sensor$co2 > 2000)], function(x) c(x, 2)) #add 2 to the flags data points with high CO2 values
  sensor$flag[which(sensor$no2 > 1000)] <- lapply(sensor$flag[which(sensor$no2 > 1000)], function(x) c(x, 3))
  sensor$flag[which(sensor$no > 300)] <- lapply(sensor$flag[which(sensor$no > 300)], function(x) c(x, 6))

  return(sensor)
}
```

```{r}
check_flags <- function(sensor, flagval, var_name){
  #plots flagged intervals
  
  index_df = create_interval_vector(sensor, flagval) #generate dataframe of flagged intervals
  
  for(pair in 1:nrow(index_df)){   # for each interval
    start_indx = index_df[pair,1] #get the start index of the interval
    end_indx = index_df[pair,2] #get the end index

    if(!is.null(start_indx) & !is.null(end_indx) & difftime(sensor$date[end_indx], sensor$date[start_indx], units="mins") < 2){ #if the interval lasts less than 2 minutes, set to NA
      sensor[[var_name]][start_indx:end_indx] <- NA
    }
    
    else{ #otherwise plot it
      
      subset_sensor <- sensor[start_indx:end_indx,]
      #cat(sprintf("start and end intervals: %s - %s \n", as.character(sensor$date[start_indx]), as.character(sensor$date[end_indx] )))
      timePlot(subset_sensor, pollutant = var_name, key = FALSE, main = paste0(as.character(sensor$date[start_indx]) ," to ", as.character(sensor$date[end_indx] )))
      
    }
    
    
  }
}

create_interval_vector <- function(sensor, flagval){
  # input a sensor dataframe and a flag (as numeric) to plot 
  # returns a dataframe that has intervals where the high values start and end 
  
  
  flagindxs <- ifelse(
    ( 
      (sensor$flag %like% flagval )
    ),
    1,  # if condition is met, put 1
    0   # else put 0
  )

  start_indx <- which(diff(c(0L, flagindxs)) == 1L)
  end_indx <- which(diff(c(flagindxs, 0L)) == -1L)
  
  mod_start_indx <- start_indx - 5 #adding 5/10 minutes before, depending on whether data is taken at 1 min or 2 min intervals
  mod_end_indx <- end_indx + 5 #adding 5/10 minutes after
  
  index_df <- data.frame(mod_start_indx,mod_end_indx) #create dataframe with corresponding start and end indices where high values happen
  
  
  #clean dataframe for intervals that start before 0
  if(any(index_df$mod_start_indx < 1)){
    
    index_df$mod_start_indx <- replace(index_df$mod_start_indx, which(index_df$mod_start_indx <1), 1 )
  }
  
  #clean dataframe for intervals that start after the end of the pollutant vector
  if(any(index_df$mod_start_indx > nrow(sensor))){

    indxs_pairs <- which(index_df$mod_start_indx > nrow(sensor))
    index_df <- index_df[!indxs_pairs]
  }
  
  #clean dataframe for intervals that end after the end of the pollutant vector
  if(any(index_df$mod_end_indx > nrow(sensor))){
    
    index_df$mod_end_indx <-replace(index_df$mod_end_indx, which(index_df$mod_end_indx > nrow(sensor)), nrow(sensor) )
  }

  #index_df <- index_df[!duplicated(index_df$mod_end_indx)]
  
  return(index_df)
  
}
```

```{r}
snfiles$sn45 <- apply_flags(snfiles$sn45)
indx_df <- create_interval_vector(snfiles$sn45,6)
check_flags(snfiles$sn45, 6, "no")
```



# Igor Quality Assurance 

Next, we'll export the data to csv, put it into Igor and do a quality inspection. 

We export using the following function: 

```{r}
# mapply(
#   write.table, #apply function write table
#   x=snfiles, file=paste(names(snfiles), "-for-igor.csv"), #for each element in snfiles, use its name to make a csv of it
#   MoreArgs=list(row.names=FALSE, sep=",")
# )
```

After checking in Igor, we'll import the data back in. 

```{r}
temp = list.files(path = "./data/quant_aq/", pattern = "^.*_NaN.*\\.csv", full.names = TRUE) #specify files we're looking for
snfiles_correction = sapply(temp, function(x) fread(file=x, data.table = TRUE) , simplify = FALSE,USE.NAMES = TRUE) #import them in

```


As an example, we'll work through sn45. 

First, we'll import all the data.

```{r}
sn45cleaning <- fread(temp, header = TRUE) #get the igor file for sn45
```

If you look at the sn45cleaning file, you'll see that "CO2" is given with uppercase C and O. We'll change it to lowercase to match the dataset. 

```{r}
sn45cleaning$"Pollutants"[which(sn45cleaning$"Pollutants" == "CO2")] <- "co2"
```

Next, we'll change the start and end vectors to be in datetime format. In order to convert, we first need to specify that "19" refers to 2019. We can do this by appending a "20" to the start and end strings. 

```{r}
sn45cleaning$Start <- paste("20", sn45cleaning$Start, sep="")
sn45cleaning$End <- paste("20", sn45cleaning$End, sep="")
```

If a start&end pair encompass only one minute, we can make sure that we get all the data from that minute by specifying that "start" begins when the minute starts (ie: 00 seconds) and ends when the minute ends (ie: 59 seconds). 
```{r}
sn45cleaning$Start <- paste(sn45cleaning$Start, "00", sep="")
sn45cleaning$End <- paste(sn45cleaning$End,"59", sep="")
```


Now we can convert the start and end columns to datetime objects. 

```{r}
sn45cleaning$Start <- ymd_hms(sn45cleaning$Start, tz = "America/New_York")
sn45cleaning$End <- ymd_hms(sn45cleaning$End, tz = "America/New_York")
```

Since the file is now formatted, we can use it to modify the data. We're going to loop through start and end interval, and change them one by one. Since there's no "all" column in the data, we'll use an if statement to specify that that means all the pollutants. 

```{r}
for(i in 1:nrow(sn45cleaning)){ #loop through igor file

  pollutant = sn45cleaning$`Pollutants`[i] #get which pollutant we're observing
  
  if (pollutant == "all"){ #if it's all
    
    snfiles$sn45[snfiles$sn45$date >= sn45cleaning$Start[i] & snfiles$sn45$date  <= sn45cleaning$End[i], c(  "co", "no" , "no2", "o3" , "pm1" , "pm25","pm10", "co2" , "no_ae", "bin0") ]  <- NA
  }
  
  else{ #otherwise
    
   snfiles$sn45[[pollutant]][snfiles$sn45$date >= sn45cleaning$Start[i] & snfiles$sn45$date  <= sn45cleaning$End[i]] <- NA
  }

}
```

Let's now put this whole process in two functions: one to format the igor files and one to modify the datasets. 

```{r}
modify_igor <- function(igor_file){
  #changing string formatting
  if(any(igor_file$Pollutants == "CO2")){
    igor_file$"Pollutants"[which(igor_file$"Pollutants" == "CO2")] <- "co2"
  }
  #formatting dates
  igor_file$Start <- paste("20", igor_file$Start, sep="")
  igor_file$End <- paste("20", igor_file$End, sep="")
  igor_file$Start <- paste(igor_file$Start, "00", sep="")
  igor_file$End <- paste(igor_file$End,"59", sep="")
  igor_file$Start <- ymd_hms(igor_file$Start, tz = "America/New_York")
  igor_file$End <- ymd_hms(igor_file$End, tz = "America/New_York")
  
  return(igor_file)
}
```

```{r}
apply_igor <- function(igor_file, snfile){
  for(i in 1:nrow(igor_file)){ #loop through igor file

  pollutant = igor_file$`Pollutants`[i] #get which pollutant we're observing
  
  if (pollutant == "all"){ #if it's all
    
    snfile[snfile$date >= igor_file$Start[i] & snfile$date  <= igor_file$End[i], c(  "co", "no" , "no2", "o3" , "pm1" , "pm25","pm10", "co2" , "no_ae", "bin0") ]  <- NA
  }
  
  else{ #otherwise
    
   snfile[[pollutant]][snfile$date >= igor_file$Start[i] & snfile$date  <= igor_file$End[i]] <- NA
  }

  }
  return(snfile)
}
```



# Final data check 

We'll check the data again using the data checking functions from before. We'll do this to make sure that the data is correct to our knowledge, before we do any analysis. 

```{r}
lapply(snfiles, function(x) data_check_test(x))
lapply(snfiles, function(x) data_check_percentiles(x))
lapply(snfiles, function(x) data_check_mean(x))
```


# Analysis 

## Polar Plots 

Polar plots are a tool that shows the relationship between pollutant concentration and wind direction and speed. In this section, we'll generate polar plots for NO, NO2, CO, CO2, bin0 and PM1. We'll do this by creating a function that will create all these polar plots, for a given sensor dataset. We'll apply this function to the snfiles list. 

```{r}
generate_overall_polarplots <- function(sn, snstring){
  #generates the polar plots for the variables we are interested in 
  polarPlot(sn, pollutant = "no", main = paste0(snstring, " NO Polar Plot", sep= " "))
  polarPlot(sn, pollutant = "no2", main = paste0(snstring, " NO2 Polar Plot", sep= " "))
  polarPlot(sn, pollutant = "co",  main = paste0(snstring, " CO Polar Plot", sep= " "))
  polarPlot(sn, pollutant = "bin0",  main = paste0(snstring, " bin0 Polar Plot", sep= " "))
  polarPlot(sn, pollutant = "pm1",  main = paste0(snstring, " PM1 Polar Plot", sep= " "))

}
```

The following chunk of code can also save the polar plots to a pdf. To do this, uncomment the "pdf" and "dev.off" lines. 

```{r}
#pdf("cleaned_sn45_polarplots.csv")
generate_overall_polarplots(snfiles$sn45, "sn45")
#dev.off()
```

We can also apply this function as a for loop to the datasets. We can also loop through the dataset names, and add those to the titles. 

```{r}
#for (i in seq(length(names(snfiles)))){
#  generate_overall_polarplots(snfiles[[i]], names(snfiles)[i])
#}
```



## Diurnal Plots 

Diurnal plots show the relationship between pollutant concentration and time (on the scale of hours, days and months). To create diurnal profiles for all of the sensors, we can apply the same method as we did to create polar plots.

```{r}
# for (i in seq(length(names(snfiles)))){
#   
#   timeVariation(snfiles[[i]], pollutant = c("no2"), main = paste0(names(snfiles)[i], " NO2 Diurnal Profile"))
#   
#   timeVariation(snfiles[[i]], pollutant = c("pm1", "co", "no",  "co2", "bin0"), normalise = TRUE, main = paste0(names(snfiles)[i], " Normalized Group of Pollutants Diurnal Profile"))
# }
```


## CorPlot 

We will make a correlation plot, using openair's corPlot function. 

```{r}
for (i in seq(length(names(snfiles)))){
  
  corPlot(snfiles[[i]], main = paste0(names(snfiles)[i], " CorPlot Profile"))

}
```


# Export 

The last thing we'll do is export the data into csvs. 

Since we want to save the datasets to files with representative names, we can use mapply (like lapply but with more functionality). For each dataset, we can set the name we used when we set snfiles as the file names. 

```{r}
# mapply(
#   write.table, #apply function write table
#   x=snfiles, file=paste(names(snfiles), "csv", sep="."), #for each element in snfiles, use its name to make a csv of it
#   MoreArgs=list(row.names=FALSE, sep=",")
# )
```

Place the exported files into the data folder.

---
title: "Little Folks Indoor/Outdoor Data Merge"
author: "Megan Ku"
---

# Description 

In this notebook, I clean the sensor data from Little Folks.

## Acknowledgements

This code is based heavily off of work done by Alli Busa.

# Environment Setup 

Import the necessary libraries. 

```{r, warning= FALSE, error = FALSE}
library(lubridate) #date and time functions 
library(data.table) #to use the data.table variable type
library(dplyr) #this library allows you to use the %>% operator
library(tidyr) #this library lets you use the complete function to account for time syncing
library(openair) #for plotting and analysis
library(stringr)
source("merge_data.R") #For importing and merging data
```

---------------------------------------------------------------------------------------------------------------------------------------------

# Prepare CPC data 

## Import CPC data 

We're going to take the CPC from every day of the experiment (each day's data is given as a TXT file), and combine in into one data frame that represents all the CPC data from a given room. 

```{r}
outdoorLF_cpc <- import_cpc_files(filepath="./Little Folks/Outdoor") 

class2_cpc <- import_cpc_files(
  filepath="./Little Folks/Class 2 - paired outdoor") 

class4_cpc <- import_cpc_files(filepath="./Little Folks/Class 4") 

LF_cpc_list <- list( "outdoor" = outdoorLF_cpc, 
                     "class2" = class2_cpc, 
                     "class4" = class4_cpc)
```
Now, we will combine the CPC data from all three locations into one list of dataframes, similar to how we created a list of dataframes in the initial walkthrough of Eastie, on line 119. We will do this so we can apply functions to the data with greater ease. 

## Setting the correct datetime

We combine the date and time columns in the cpc data to create datetime objects. 
```{r}
LF_cpc_list <- lapply(LF_cpc_list, set_correct_datetime)
```

## Cleaning the data for non-numerical errors 

I had found that the data isn't always imported in the right format (ie as strings instead of numeric), which leads to problems when you want to perform operations on those data. In order to solve this, we are checking the column type of each dataset, determining where there are outliers, and fixing the formatting issues. 

We'll check the data format of each column in each data set. Numeric data should be "double" or "integer".

```{r}
sapply(LF_cpc_list, function(x) sapply(x,typeof))
```

## Cleaning the data for numerical errors 

Now, we'll clean the data according the guidelines we set in place in our 'Expected Values' spreadsheet. 

### Discard all zero and negative values 

```{r}
lapply(LF_cpc_list, function(x) sprintf("Number of negative and zero values: %d", sum(x$concent <= 0, na.rm = TRUE)))
```
Now, we can clean the rest. 

```{r}
LF_cpc_list <- sapply(LF_cpc_list, remove_cpc_zero)
```

Let's also check how many values are NA: 

```{r}
lapply(LF_cpc_list, function(x) sprintf("Number of NA values: %d", sum(is.na(x$concent))))
lapply(LF_cpc_list, function(x) sprintf("Percentage of NA values: %f", 100*(sum(is.na(x$concent)) / nrow(x))))
```
### Check low and high values 

First, we'll check how many values are below 1000: 

```{r}
lapply(LF_cpc_list, function(x) sprintf("Number of values below 1000 particles/cm3: %d", sum(x$concent < 1000, na.rm = TRUE)))
lapply(LF_cpc_list, function(x) sprintf("Percentage of values below 1000 particles/cm3: %f", 100*(sum(x$concent < 1000, na.rm = TRUE)/ nrow(x))))
```
Next, we'll check how many values are above 100000: 

```{r}
lapply(LF_cpc_list, function(x) sprintf("Number of values above 100000 particles/cm3: %d", sum(x$concent > 100000, na.rm = TRUE)))
lapply(LF_cpc_list, function(x) sprintf("Percentage of values above 100000 particles/cm3: %f", 100*sum(x$concent > 100000, na.rm = TRUE)/ nrow(x)))
```

---------------------------------------------------------------------------------------------------------------------------------------------

# Preparing the PM data 

## Importing the PM data 

Now we will import all the PM data, using the same method as for the CPC data. We will combine the three PM data frames into one list of dataframes. Lastly, we will clean all the dataframes in the list of dataframes using the function above. 

```{r}
# import pm data 

LF_outdoor_PM <- import_pm_files(
  filepath="./Modulair-PM Data", regex_pattern="114.*.csv")

LF_class1_PM <- import_pm_files(
  filepath="./Modulair-PM Data", regex_pattern="104.*.csv")

LF_class2_PM <- import_pm_files(
  filepath="./data", regex_pattern = "data.mod-pm-00126.final.csv")

LF_class3_PM <- import_pm_files(
  filepath="./Modulair-PM Data", regex_pattern="125.*.csv")

LF_class4_PM <- import_pm_files(
  filepath="./Modulair-PM Data", regex_pattern="099.*.csv")

LF_admin_PM <- import_pm_files(
  filepath="./Modulair-PM Data", regex_pattern="111.*.csv")

# combine PM data 

LF_pm_list = list("outdoor" = LF_outdoor_PM,
                  "class1" = LF_class1_PM,
                  "class2" = LF_class2_PM,
                  "class3" = LF_class3_PM,
                  "class4" = LF_class4_PM,
                  "admin" = LF_admin_PM)
```

## Setting the correct datetime

The first step for this subsection is to clean the PM data. This involves setting it to a datetime object, changing the timezone from UTC to EST/EDT, and rounding the datetime object so that the data can be matched with the CPC data, minute by minute. 

Now, we'll apply it. 

```{r}
LF_pm_list <- lapply(LF_pm_list,  function(x) clean_PM_data(x)) # convert pm data to local time, round date 
```

## Cleaning the data for non-numerical errors

As before, we'll check for non-numerical errors. 

```{r}
sapply(LF_pm_list, function(x) sapply(x,typeof))
```

Everything here looks okay, for our purposes.

## Cleaning the data for numerical errors 

According to our spreadsheet, pm1 shouldn't exceed 1 microgram/m3, and should never be negative. 

```{r}
lapply(LF_pm_list, function(x) sprintf("Number of negative values: %d", sum(x$pm1 < 0, na.rm = TRUE)))
```
There are no negative values. 

---------------------------------------------------------------------------------------------------------------------------------------------

# Merging 

## Averaging CPC data 

In order to merge with the PM data, we have to get the data to have the same time intervals. This way, we will be able to match datapoints based on what time they were taken. Since the PM data has a smaller resolution than the CPC data (1 min opposed to 1 sec), we will average the CPC data so that it will be on a 1 minute time basis. 

To do this, we will create a date1min vector in the CPC data; this vector gives the datetime rounded to the nearest minute for each datapoint. Then we will take all of the numeric data, group them into categories so that each category has the same date1min, and then taking the mean of each group. 

Let's check that all the columns are in the right format, again. 

```{r}
sapply(LF_cpc_list, function(x) sapply(x,typeof))
```
And making sure there aren't any rows that could stop us from taking the mean: 

```{r}
sapply(LF_cpc_list, function(x) which(is.na(x$date)))
```
Since there aren't any issues, we can apply the function.
```{r}
LF_cpc_list2 <- lapply(LF_cpc_list, average_cpc)
```


## Merging Function 

The function we are using is also based on line 74 of the initial analysis walkthrough. 

```{r}
LF_rooms <- sapply(names(LF_cpc_list), function(k) merge(LF_cpc_list2[[k]], LF_pm_list[[k]], by.x = "date1min", by.y = "date", all = TRUE), simplify = FALSE,USE.NAMES = TRUE)
```

---------------------------------------------------------------------------------------------------------------------------------------------

# Apply Correction Factor 

As stated in the HEPA Analysis powerpoint, here we will "multiply CPC concentrations by a static value to account for noise from the sensors." We do this by creating a function, which takes in a dataframe and a correction factor. It multiplies that dataframe's concent vector by the input correction factor.  

The correction data is based on which sensor is used. Below, we list which sensor is used in each location.

015 - class2
016 - outdoor
077 - class4

We apply that function to all of the dataframes:
```{r}
LF_rooms_previous = LF_rooms # replicating the list, to check the function

LF_rooms["class2"] <- apply_correction(LF_rooms["class2"], 0.9449454)

LF_rooms["outdoor"] <- apply_correction(LF_rooms["outdoor"], 1.005473)

LF_rooms["class4"] <- apply_correction(LF_rooms["class4"],  1.100486)
```

We can check that the concentration vector still stays the same: 

```{r}
all(LF_rooms_previous$class2$concent == LF_rooms$class2$concent, na.rm = TRUE)
all(LF_rooms_previous$outdoor$concent == LF_rooms$outdoor$concent, na.rm = TRUE)
all(LF_rooms_previous$class4$concent == LF_rooms$class4$concent)
```


---------------------------------------------------------------------------------------------------------------------------------------------

# Generating indoor/outdoor ratios 

We will create a function that merges two dataframes (a dataframe from a testing location inside, and from outside).

We'll run this function on the datasets. 

```{r}
LF_class1_merged <- ratio_merge(LF_pm_list$class1, LF_rooms$outdoor, cpc = FALSE)
LF_class2_merged <- ratio_merge(LF_rooms$class2, LF_rooms$outdoor, cpc = TRUE)
LF_class3_merged <- ratio_merge(LF_pm_list$class3, LF_rooms$outdoor, cpc = FALSE)
LF_class4_merged <- ratio_merge(LF_rooms$class4, LF_rooms$outdoor, cpc = TRUE)

LF_admin_merged <- ratio_merge(LF_pm_list$admin, LF_rooms$outdoor, cpc = FALSE)
```

```{r}
# Class 2
LF_class2_merged <- LF_class2_merged %>%
  filter(date1min > as.POSIXct("2021-03-24 09:00:00", tz='America/New_York'),
         date1min < as.POSIXct("2021-04-07 07:08:00", tz='America/New_York'))

# Class 4
LF_class4_merged <- LF_class4_merged %>%
  filter(date1min > as.POSIXct("2021-03-24 09:28:00", tz='America/New_York'), 
         date1min < as.POSIXct("2021-04-07 07:21:00", tz='America/New_York'))

# Admin
LF_admin_merged <- LF_admin_merged %>%
  filter(date1min > as.POSIXct("2021-03-24 09:30:00", tz='America/New_York'), 
         date1min < as.POSIXct("2021-04-07 07:29:00", tz='America/New_York'))

# Class 1
LF_class1_merged <- LF_class1_merged %>%
  filter(date1min > as.POSIXct("2021-03-24 09:15:00", tz='America/New_York'), 
         date1min < as.POSIXct("2021-04-07 07:29:00", tz='America/New_York'))

# Class 3
LF_class3_merged <- LF_class3_merged %>%
  filter(date1min > as.POSIXct("2021-03-24 09:34:00", tz='America/New_York'), 
         date1min < as.POSIXct("2021-04-07 07:27:00", tz='America/New_York'))

```


---------------------------------------------------------------------------------------------------------------------------------------------

# Exporting csvs

Make a list of the new dataframes: 

```{r}
merged <- list("LF_class1_merged" = LF_class1_merged,
               "LF_class2_merged" = LF_class2_merged,
               "LF_class3_merged" = LF_class3_merged,
               "LF_class4_merged" = LF_class4_merged,
               "LF_admin_merged" = LF_admin_merged)
```

Export them:

```{r}
mapply(
  write.table, #apply function write table
  #for each dataframe, use its name to make a csv of it
  x=merged, file=paste("data", paste(names(merged), "csv", sep="."), sep="/"), 
  MoreArgs=list(row.names=FALSE, sep=",")
)
```

---------------------------------------------------------------------------------------------------------------------------------------------





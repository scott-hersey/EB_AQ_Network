---
title: "R Notebook"
---


# Importing Packages 

```{r}
library(tidyr)
library(data.table)
library(lubridate)
library(dplyr)
library(openair)
library(baseline)
```


## Set Up Meterological Data 

```{r}
format_METtime <- function(metdata){
  metdata$date <- as.POSIXct(metdata$valid, format = "%Y-%m-%d %H:%M", tz = "America/New_York") #setting datetime, using correct timezone
  metdata <- metdata %>%
    setnames(old = c("drct", "sped"), new = c("wd", "ws")) %>% #rename
    complete(date = seq(from = min(date), to= max(date),  by = "1 min")) %>%#make 1 minute intervals
    fill(c("wd", "ws")) #fill those new 1 minute interval rows 
    
  metdata <- metdata[!grepl("null", metdata$ws),] #getting rid of met's null data
  metdata <- metdata[!grepl("null", metdata$wd),]
  metdata$wd <- as.numeric(metdata$wd)  #reformatting so it's integers instead of strings
  metdata$ws <- as.numeric(metdata$ws)
  metdata$ws <- metdata$ws * (1609/3600) #converting to m/s
  metdata[,c("tmpc", "valid", "station")] <- list(NULL) #getting rid of unnecessary variables
  
  return(metdata)
}
```

```{r}
metdata = read.csv("data/metdata.csv", header=TRUE, fileEncoding="UTF-8-BOM") #import meteorology file
metdata <- format_METtime(metdata)
setDT(metdata, key = "date")
```




## Create Dataframes

### Additional Functions Based on Testing 

I created the function below, because, through testing, I found that when the bin values should be negative, they're actually all zero. This function corrects that: 

```{r}
clean_bins <- function(sn){
  sn[which(sn$bin0 == 0 & 
             sn$bin1 == 0 & 
             sn$bin2 == 0 & 
             sn$bin3 == 0 & 
             sn$bin4 == 0 & 
             sn$bin5 == 0), c("bin0", "bin1", "bin2", "bin3", "bin4", "bin5")] <- NA
  snfilt <- sn 
  return(snfilt)
}
```

### Creating Dataframes

In this section, I use the data cleaning steps (in the final_data_cleaning function) to create datasets for the sensors.

```{r}
sn45 = read.csv("Final_Customer/20200519_final_045.csv",  header=TRUE, fileEncoding="UTF-8-BOM") #importing file that Eben sent via dropbox
sn45no2model <- read.csv("Final_Customer/no2_HYBRID_045.txt") #also file from dropbox

source("functions/final_data_cleaning.R") #run the filtering function
sn45 <- final_data_cleaning(sn45, metdata, sn45no2model)
source("functions/bincorrection.R") #change 15 bins to 6 
sn45 <- change_bins(sn45)
sn45 <- clean_bins(sn45) 
write.csv(sn45, "sn45filt.csv", row.names = FALSE)
```

```{r}
rm(sn45) #to save memory, delete already created dataframes
rm(sn45no2model)
gc()

sn49 = read.csv("Final_Customer/20200519_final_049.csv",  header=TRUE, fileEncoding="UTF-8-BOM")
sn49no2model <- read.csv("Final_Customer/no2_HYBRID_049.txt") 

sn49 <- final_data_cleaning(sn49, metdata, sn49no2model)
source("functions/bincorrection.R")
sn49 <- change_bins(sn49)
sn49 <- clean_bins(sn49)

#check suspicious CO spike  Oct 14 to oct 21
timePlot(selectByDate(sn49, start = "14/10/2019", end = "21/10/2019"), pollutant = "co")
#pdf("sn49-suspicious-CO-spike.pdf")
#timePlot(selectByDate(sn49, start = "17/10/2019", end = "17/10/2019"), pollutant = "co")
#dev.off()
sn49[which(sn49$date == "2019-10-17 06:00:00"): which(sn49$date == "2019-10-17 15:50:00"), "co"] <- NA #removing spike
write.csv(sn49, "sn49filt.csv", row.names = FALSE)
```

```{r}
rm(sn49)
rm(sn49no2model)
gc()

sn62 = read.csv("Final_Customer/20200519_final_062.csv",  header=TRUE, fileEncoding="UTF-8-BOM")
sn62no2model <- read.csv("Final_Customer/no2_HYBRID_062.txt") 
sn62 <- final_data_cleaning(sn62, metdata, sn62no2model)
sn62 <- change_bins(sn62)
sn62 <- clean_bins(sn62)
#Check for first few days , suspicious activity
timePlot(selectByDate(sn62, start = "11/9/2019", end = "11/9/2019"), pollutant = c("wd", "ws", "co", "o3"  , "no", "correctedNO", "no2model", "bin0", "bin1", "bin2", "bin3", "bin4", "bin5"), y.relation = "free")
timePlot(selectByDate(sn62, start = "11/9/2019", end = "11/9/2019"), pollutant = "co")
sn62 <- sn62[which(sn62$date == "2019-09-11 18:00:00"): which(sn62$date == tail(sn62$date, n = 1)), ] #remove those days
tz(sn62$date) <- "America/New_York" #making sure timezone didn't change
write.csv(sn62, "sn62filt.csv", row.names = FALSE)
```

```{r}
rm(sn62)
rm(sn62no2model)

sn67 = read.csv("Final_Customer/20200519_final_067.csv",  header=TRUE, fileEncoding="UTF-8-BOM")
sn67no2model <- read.csv("Final_Customer/no2_HYBRID_067.txt") 

sn67 <- final_data_cleaning(sn67, metdata, sn67no2model)
sn67 <- change_bins(sn67)
sn67 <- clean_bins(sn67)
#checking and removing suspicious peaks
timePlot(selectByDate(sn67, start = "7/2/2020", end = "9/2/2020"), pollutant = c("o3"  , "no", "correctedNO"))
sn67[which(sn67$date == "2019-10-07 12:00:00"): which(sn67$date == "2020-02-09 00:00:00"), "co"] <- NA
sn67[which(sn67$date == "2020-02-07 12:00:00"): which(sn67$date == "2020-02-09 00:00:00"), "o3"] <- NA
indx1 <- which(sn67$date == "2019-10-13 07:31:00") 
indx2 <- which(sn67$date == "2019-10-13 08:01:00")
plot(sn67$date[indx1:indx2], sn67$co[indx1:indx2], type = "l")
sn67[indx1:indx2, "co"] <- NA

write.csv(sn67, "sn67filt.csv", row.names = FALSE)
```
I recorded the peak I wasn't sure about with this code:
```{r}
pdf("notsure-co-spike-sn67.pdf")
timePlot(selectByDate(sn67, start = "13/10/2019", end = "13/10/2019"), pollutant = "co")
dev.off()
```


Note that SN 72 data cleaning isn't currently working.
```{r}
rm(sn67)
rm(sn67no2model)

sn72 = read.csv("Final_Customer/20200519_final_072.csv",  header=TRUE, fileEncoding="UTF-8-BOM")
sn72no2model <- read.csv("Final_Customer/no2_HYBRID_072.txt") 

sn72 <- final_data_cleaning(sn72, metdata, sn72no2model)
write.csv(sn72, "sn72.csv", row.names = FALSE)
```

## Final Data Cleaning Function 

Even though I call the final data cleaning function from a separate file above, I also include it here for testing and debugging purposes. 

```{r}
sensor <- sn72 
no2model <- sn72no2model

## PREPARE DATES and DATAFRAMES

sensor$date <- ymd_hms(sensor$timestamp, tz="UTC") #parse datetime
sensor$date <- with_tz(sensor$date, "America/New_York")
sensor[, c("X1","X", "sn" )] <- list(NULL) #removing unused variables
sensor <- mutate(sensor, originaldate = date) #keeping original times for comparing to flight data
sensor$date <- round_date(ymd_hms(sensor$date, tz="America/New_York"), unit="minute") #round date for merging
sensor<- sensor[order(sensor$originaldate),] #put it in chronological order
setDT(sensor, key = "date") #change object type to data.table
sensor <- unique(sensor, by = "date") #remove duplicates



no2model <- no2model[order(no2model$timestamp),] #putting it in chronological order 
no2model$date <-  as_datetime(no2model$timestamp , tz="UTC",  origin = (sensor$originaldate[1] - no2model$timestamp[1])) #getting the origin from the sensor
no2model$date <- with_tz(no2model$date, "America/New_York") #convert timezone
no2model$date <- round_date(no2model$date, unit = "minute")   #to merge with finaldf
#tz(no2model$date) <- "America/new_York"
no2model$timestamp <- NULL #excluding this variable

names(no2model)[names(no2model)=="no2"] <- "no2model"#renaming, for the merge
setDT(no2model, key = "date") # change to data.table object
no2model <- unique(no2model, by = "date") #remove duplicates


joined.sn.no2 <- no2model[sensor, nomatch = 0] #first join
finaldf <- metdata[joined.sn.no2, nomatch = 0] #second join

sensor <- finaldf #rename 
sensor$o3 <- replace(sensor$o3, sensor$o3 < 0, 0) #filter for O3

# NO AE Filter 

#create a vector that shows the derivatives 
no_ae_derivative <- c(0,diff(sensor$no_ae, na.rm = TRUE))
#creating logical vector to pick which things to get rid of 
logical_vec <- abs(no_ae_derivative) < abs(2.5*(sd(no_ae_derivative, na.rm=TRUE)))

#make sure SN NO values that == 0 are not removed
if (any(sensor$no == 0, na.rm = TRUE)){
  sensor$no <- replace(sensor$no, 0, 0.00001)
}
# if logical_vec is true, then set the NO value to NA 
sensor$no <- sensor$no * logical_vec
sensor$no <- replace(sensor$no, 0, NA)
  
  
# NO BASELINE CORRECTION 

tz(sensor$date) <- "America/New_York"
# create day column
sensor[, day := as.Date(date, format="%Y-%m-%d", tz = "America/New_York")]
# create corrected column 
sensor[, correctedNO := seq(0,0,length.out= length(sensor$no))]
sensor$correctedNO[sensor$correctedNO == 0] <- NA  #set them actually to NA

dropNAsensor<- sensor[!is.na(sensor$no), ] # drop NO NAs

unique_days <- c(unique(dropNAsensor$day, na.rm=TRUE)) #get all of the unique days in the sensor

for (i in 2:(length(unique_days)-1)){ #for all days
  temp <- subset(dropNAsensor, day %in% unique_days[i], c("day", "no", "date")) #create temp dataset

  if (nrow(temp) > 550){
    wholebase.peakDetection <- baseline(t(temp$no), method='peakDetection',left=50, right=50, lwin=10, rwin=10) #baseline correction
  
  #replace the correctedNO column values with the baseline correction from earlier
  dropNAsensor$correctedNO[which(dropNAsensor$date == temp$date[1]): which(dropNAsensor$date == tail(temp$date, n=1))] <-    c(getCorrected(wholebase.peakDetection))
  }
  
  else{
    if (sum(temp$no < 0, na.rm = TRUE) / nrow(temp) < 0.25){
      dropNAsensor$correctedNO[which(dropNAsensor$date == temp$date[1]): which(dropNAsensor$date == tail(temp$date, n=1))] <-    
        sensor$no[which(sensor$date == temp$date[1]): which(sensor$date == tail(temp$date, n=1))]
    }

  }
  
}

sensor$correctedNO[which(sensor$date %in% dropNAsensor$date)] <- dropNAsensor$correctedNO # replace values based on date 

```

# Creating Graphs

Below I include the code I wrote to check some of the data cleaning results


```{r}
sensor <- sn45
#pdf("SN45_filtered.pdf")
timePlot(selectByDate(sensor, start = "9/9/2019", end = "9/11/2019"), pollutant = c("wd", "ws", "no", "no2model", "co", "co2",  "co2diff",  "o3", "rh_manifold", "temp_manifold"), y.relation = "free")

timePlot(selectByDate(sensor, start = "9/11/2019", end = "1/1/2020"), pollutant = c("wd", "ws", "no", "no2model", "co", "co2",  "co2diff", "o3", "rh_manifold", "temp_manifold"), y.relation = "free")


timePlot(selectByDate(sensor, start = "2/1/2020", end = "1/4/2020"), pollutant = c("wd", "ws", "no", "no2model", "co", "co2",  "co2diff",  "o3", "rh_manifold", "temp_manifold"), y.relation = "free")

timePlot(selectByDate(sensor, start = "1/4/2020", end = "15/5/2020"), pollutant = c("wd", "ws", "no", "no2model", "co", "co2",  "co2diff", "o3", "rh_manifold", "temp_manifold"), y.relation = "free")

#dev.off()
```

Including sub-dataframes from the cleaning function. This one checks that baseline function worked :
```{r}
timePlot(selectByDate(dropNAsensor, start = "9/9/2019", end = "9/11/2019"), pollutant = c("correctedNO", "no"))
```

Using these graphs, I tested whether O3 drops when NO has a peak: 
```{r}
timePlot(selectByDate(sensor,start = "2/1/2020", end = "2/1/2020"), pollutant = c("o3", "no", "temp_manifold"), y.relation = "free")
timePlot(selectByDate(sensor,start = "2/2/2020", end = "2/2/2020"), pollutant = c("no","o3", "temp_manifold"), y.relation = "free")
timePlot(selectByDate(sensor,start = "2/3/2020", end = "3/3/2020"), pollutant = c("no","o3", "temp_manifold"), y.relation = "free")
timePlot(selectByDate(sensor,start = "2/4/2020", end = "3/4/2020"), pollutant = c("no","o3", "temp_manifold"), y.relation = "free")
timePlot(selectByDate(sensor,start = "11/5/2020", end = "13/5/2020"), pollutant = c("no","o3", "temp_manifold"), y.relation = "free")
timePlot(selectByDate(sensor,start = "2/2/2020", end = "6/2/2020"), pollutant = "o3", y.relation = "free")
timePlot(selectByDate(sensor,start = "2/3/2020", end = "6/3/2020"), pollutant = "o3", y.relation = "free")
```

With these graphs, I checked if O3 has an afternoon peak:
```{r}
timeVariation(sensor, pollutant = "o3", local.tz = "America/New_York")
timeVariation(sensor, pollutant = "o3", type = "season")
#pdf("diurnal-o3-trends.pdf")
timeVariation(sensor, pollutant = "o3")
#dev.off()
tz(sensor$date) # checking time zone of the sensor
```




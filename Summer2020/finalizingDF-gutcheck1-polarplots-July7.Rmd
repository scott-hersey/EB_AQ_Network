---
title: "Finalizing dataframes, gutchecking the data and creating some polar plots"
---

In this notebook, I'll be finalizing the dataframes, and exporting them to csvs. I'll also be gut checking those dataframes, and making sure that they look how we expect. I'll create the polar plots that exhibit overall behavior. Finally, I'll start working on the scripts that create the regime dataframes. 

## Importing Libraries 

```{r message=FALSE, warning=FALSE}

library(tidyr)
library(dplyr)
library(lubridate) #setting dates for df
library(data.table) #to setnames
library(openair)

```

# Finalizing Dataframes 

First, I'll take the code I was using yesterday to create the dataframe and make it into a function : 

Actually, so we don't always have to reimport the met data, I'll create a separate function to turn that into the correct dataframe

```{r}
createmetdf <- function(metdatapath){
  #metdatapath is the name of the file
  
  metdata = read.csv(metdatapath, header=TRUE, fileEncoding="UTF-8-BOM") #import meteorology file
  
  metdata$date <- as.POSIXct(metdata$valid, format = "%Y-%m-%d %H:%M", tz = "America/New_York") #setting datetime, using correct timezone
  metdata <- metdata %>%
    setnames(old = c("drct", "sped"), new = c("wd", "ws")) %>% #rename
    complete(date = seq(from = min(date), to= max(date),  by = "1 min")) %>%#make 1 minute intervals
    fill(c("wd", "ws")) #fill those new 1 minute interval rows 
    
  metdata[!grepl("null", metdata$ws),] #getting rid of met's null data
  metdata[!grepl("null", metdata$wd),]
  metdata$wd <- as.numeric(metdata$wd)  #reformatting so it's integers instead of strings
  metdata$ws <- as.numeric(metdata$ws)
  metdata$ws <- metdata$ws * (1609/3600) #converting to m/s
  metdata[,c("tmpc", "valid", "station")] <- list(NULL) #getting rid of unnecessary variables
  
  return(metdata)
}

```

```{r}
createsensordf <- function(sensorpath, no2modelpath, metdf){
  #the sensor path is the name of the sensor data 
  #no2modelpath is the name of the no2model data 
  
  sensor = read.csv(sensorpath, header=TRUE, fileEncoding="UTF-8-BOM") #import sensor file
  #getting our sensor data ready
  sensor$date <- ymd_hms(sensor$date, tz="America/New_York") #parse datetime
  sensor[, c("lat", "long", "X", "X.1")] <- list(NULL) #removing columns for speed
  mutate(sensor, originaldate = date) #keeping original times for comparing to flight data
  sensor$date <- round_date(sensor$date, unit="minute")
  
  #merging
  finaldf <- merge(x = sensor, y = metdf, by = "date", all = FALSE) #all=FALSE gets rid of datapoints if one of the two datasets don't have them
  
  no2model <- read.csv(no2modelpath) 
  # the timestamp is given as seconds from some origin, so I'll deduce the origin
  no2model$date <-  as_datetime(no2model$timestamp , origin = (sensor$originaldate[1] - no2model$timestamp[1])) 
  no2model$date <- round_date(no2model$date, unit = "minute")   #to merge with finaldf
  no2model$timestamp <- NULL #excluding this variable
  
  names(no2model)[names(no2model)=="no2"] <- "no2model"#renaming, for the merge
  
  finaldf <- merge(x = finaldf, y = no2model, by = "date", all = FALSE)
  
  return(finaldf)
  
}
```


I'm also putting these functions into separate scripts (will be available on github), so it can be called from other notebooks. 


Next, I'll be running that function on all the sensor data. Note that I have all the data files in the same folder as my Rproject (the github repo folder), but I'm not pushing them to github, because of how much space they take up. 


```{r}
source("createsensordf.R")
metdata <- createmetdf("metdata.csv")

sn45 <- createsensordf("Final_Customer/20200519_final_045.csv", "Final_Customer/no2_HYBRID_045.txt", metdata)
sn49 <- createsensordf("Final_Customer/20200519_final_049.csv", "Final_Customer/no2_HYBRID_049.txt", metdata)
start_time <- Sys.time()
sn62 <- createsensordf("Final_Customer/20200519_final_062.csv", "Final_Customer/no2_HYBRID_062.txt", metdata)
end_time <- Sys.time()
(end_time - start_time)
sn67 <- createsensordf("Final_Customer/20200519_final_067.csv", "Final_Customer/no2_HYBRID_067.txt", metdata)
sn72 <- createsensordf("Final_Customer/20200519_final_072.csv", "Final_Customer/no2_HYBRID_072.txt", metdata)


```



Lastly, I'll write all of these files to csv: 

```{r}
write.csv(metdata, "cleaned_metdata.csv")

write.csv(sn45, "sn45.csv")
write.csv(sn49, "sn49.csv")
start_time <- Sys.time()
write.csv(sn62, "sn62.csv")
end_time <- Sys.time()
(end_time - start_time)
write.csv(sn67, "sn67.csv")
write.csv(sn72, "sn72.csv")

```

I'll now upload these files to a dropbox folder. 


# Gutcheck the data

I had originally just started making plots and generating summary statistics. But, in order to save time, and to be more organized, I'm going to read all of the csvs back in and create functions. 

Note: I read this back in one at a time, and then did each analysis for them, instead of importing them in all at once. I did this, so that I could delete the dataset after I used it, and that way, save memory. 

```{r}
sn67 <- read.csv("sn67.csv")
sn67$X <- NULL
sn67$date <- ymd_hms(sn67$date)

sn62 <- read.csv("sn62.csv")
sn62$X <- NULL
sn62$date <- ymd_hms(sn62$date)

sn49 <- read.csv("sn49.csv")
sn49$X <- NULL
sn49$date <- ymd_hms(sn49$date)

sn45 <- read.csv("sn45.csv")
sn45$X <- NULL
sn45$date <- ymd_hms(sn45$date)

```

```{r}
generate_summary <- function(sn){
  summarystats <- sn %>%
    mutate(month = format(date, "%m"), year = format(date, "%y")) %>%
    group_by(month, year) %>%
    summarise(
      count = n(), 
      monthly_avg_pm1 = mean(pm1 , na.rm = TRUE),
      monthly_avg_pm25 = mean(pm25, na.rm = TRUE),
      monthly_avg_temp = mean(temp_manifold, na.rm = TRUE),
      monthly_avg_ws = mean(ws, na.rm = TRUE),
      monthly_avg_co2 = mean(co2, na.rm = TRUE),
      monthly_avg_no2model = mean(no2model, na.rm = TRUE),
      monthly_avg_no = mean(no, na.rm = TRUE),
      monthly_std_pm1 = sd(pm1 , na.rm = TRUE),
      monthly_std_pm25 = sd(pm25, na.rm = TRUE),
      monthly_std_temp = sd(temp_manifold, na.rm = TRUE),
      monthly_std_ws = sd(ws, na.rm = TRUE),
      monthly_std_co2 = sd(co2, na.rm = TRUE),
      monthly_std_no2model = sd(no2model, na.rm = TRUE),
      monthly_std_no = sd(no, na.rm = TRUE))
  return(summarystats)
}
```

```{r}
plotTrends <- function(sn){
    timePlot(sn[1:((1/3)*length(sn$date)),], pollutant = c("wd", "ws", "no2model", "no", "rh_manifold", "co", "co2", "pm1", "pm25", "pm10"), y.relation = "free")
    timePlot(sn[((1/3)*length(sn$date)): ((2/3)*length(sn$date)),], pollutant = c("wd", "ws", "no2model", "no", "rh_manifold", "co", "co2", "pm1", "pm25", "pm10"), y.relation = "free")
    timePlot(sn[((2/3)*length(sn$date)):length(sn$date),], pollutant = c("wd", "ws", "no2model", "no", "rh_manifold", "co", "co2", "pm1", "pm25", "pm10"), y.relation = "free")
  
}
```


### Analysis of sn72

```{r}
plotTrends(sn72)
```
It looks like September 17 (or close to that date), there's a weird linear trend through a lot of the variables. The same is true for Oct 31, except this one has less of an impact on NO. 

Also, CO2 has a changing baseline and exhibits some periodicity. It might be meant to look like this, if CO2 is more effected by meteorological trends and/or other sources of pollution. RH exhibits periodicity as well, but that was to be expected. 

And create summary statistics: 

```{r}
(summary72 <- generate_summary(sn72))
```

It looks like NO is negative!

```{r}
timePlot(sn72, pollutant="no")
```

Yeah, there are negative values. So, I can filter everything, so it's more than zero, but I think we should talk about this too. 


### Analysis of sn67

```{r}
(summary67 <- generate_summary(sn67))
```

```{r}
plotTrends(sn67)
```

I have the same thoughts about these sets of plots as the last one, except that there is only a linear trend around Oct 31-Nov 1 and not during September 17.

### Analysis of sn62 

```{r}
plotTrends(sn62)
```

For this sensor, there's a huge CO spike initially, at September 14. Also, CO2 starts really high, and its baseline gradually gets lower. It looks like there might be a suspicious linear trend at Oct 15, but not Oct 31, like the other ones. 

```{r}
(summary62 <- generate_summary(sn62))
```
We see the same trends in this data as in the last two sensors. 

### Analysis for sn 49 

```{r}
plotTrends(sn49)
```
I see : 
- a moving CO2 baseline 
- a linear trend around Oct 30

```{r}
(summary49 <- generate_summary(sn49))
```

### Analysis for sn 45 

```{r}
plotTrends(sn45)
```
This sensor seems to have a long, peculiar linear trend, that happens around the first half of April! 

There's a lot more oscillation in CO2 in the third period than the first and second. Also, I can see by this graph that around January 1st, the CO2 count goes negative. Also, this sensor has the most constant CO2 values (it looks like from the graph at least), and the baseline doesn't appear to change drastically. To confirm this, I'll plot the whole timeseries below:

```{r}
timePlot(sn45, pollutant="co2")
```
I would say the baseline changes a bit, but not drastically, from this graph.

```{r}
(summary45 <- generate_summary(sn45))
```

```{r}
( max(summary45$monthly_std_co2) < max(summary67$monthly_std_co2)) 
(max(summary45$monthly_std_co2) < max(summary49$monthly_std_co2) )
(max(summary45$monthly_std_co2) < max(summary62$monthly_std_co2) ) 
 (max(summary45$monthly_std_co2) < max(summary72$monthly_std_co2) )
```

The max standard deviation of CO2 from this sensor is lower than the other sensors


# Cleaning based on the results from above 

So, it looks like there are periods of inconsistent data. For now, I will get rid of these datapoints. 

For example, I will remove any concentration datapoint that is negative. 

```{r}
replace_negvals <- function(sn){
  sn[c("wd", "ws", "co", "co2", "no2model", "no")] <- replace(sn[c("wd", "ws", "co", "co2", "no2model", "no")], sn[c("wd", "ws", "co", "co2", "no2model", "no")] < 0, 0)
  return(sn)
}

sn45 <- replace_negvals(sn45)
sn49 <- replace_negvals(sn49)
sn62 <- replace_negvals(sn62)
sn67 <- replace_negvals(sn67)
sn72 <- replace_negvals(sn72)
```

# Overall Trends

```{r}
generate_overall_polarplots <- function(sn){
  #generates the polar plots for the variables we are interested in 
  polarPlot(sn, pollutant = "no")
  polarPlot(sn, pollutant = "no2model")
  polarPlot(sn, pollutant = "co")
  polarPlot(sn, pollutant = "co2")
}

```

```{r}
generate_overall_polarplots(sn45)
```
This seems like what we were expecting. The high concentrations in no, no2 and co come from the directions of 22L and 22 R. It looks like there are higher CO2 concentrations from the North, however. North of this site is road 145, a repair shop and a residential area. If there is increased CO2 from the North, and it's caused by a different pollutant, than I'm surprised that it didn't pick up high concentrations from the East as well (I-90).

```{r}
generate_overall_polarplots(sn49)
```
For context, the airport surrounds the site from the East to the South (with a bay between them). The site is at the very edge of Winthrop, and is surrounded by residential area to the North. 
Is this correct? There's high CO2 from all over? 

```{r}
generate_overall_polarplots(sn62)
```

Immediately to the Southeast of this site is a Navy Fuel Pier. The airport surrounds the site from the North, to the East, to the South (but doesn't directly touch it). The airport is closest to the site in the SE direction, but I-90 and a bay is between them.

```{r}
generate_overall_polarplots(sn67)
```

This site is on a residential island. It is closest to runway 27, at about ~ 260 degrees. 

```{r}
generate_overall_polarplots(sn72)
```
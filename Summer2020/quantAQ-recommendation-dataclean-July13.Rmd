---
title: "Cleaning Data Based on QuantAQ recommendations"
---

# Overview 

In this notebook, I will clean SN 45 based on the recommendations from Eben and David. Specifically, I will change all negative O3 values to zero, apply a no_ae filter to NO data and also do more sophisticated filtering on NO. 

# Setup 

First, I'll import the correct libraries, as well as the sensor data. 

```{r warning=FALSE}
library(lubridate)
library(openair)
library(data.table)
library(dplyr)
```

```{r}
sn45 <- read.csv("data/sn45.csv")
sn45$X <- NULL 
sn45$date <- ymd_hms(sn45$date, tz="America/New_York")
```

# Record of Data before cleaning 

Before I clean this data, I will first create some "before" graphs, to later show before and after pictures of the data and  the effect the filters had. 

In addition to these, I had originally put timePlots of o3, co, co2, no and no2 of the whole duration of the data (split into 5) on the Google Drive. I will also be referencing those plots.

```{r}

timePlot(selectByDate(sn45, start = "20/2/2020", end = "5/3/2020"), pollutant = "o3", main = "O3 vs Time for SN45")
timePlot(selectByDate(sn45, start = "30/11/2019", end = "15/12/2019"), pollutant = "co", main = "co vs Time for SN45")
timePlot(selectByDate(sn45, start = "14/2/2020", end = "15/2/2020"), pollutant = "no", main = "NO vs Time for SN45")
timePlot(selectByDate(sn45, start = "24/2/2020", end = "29/2/2020"), pollutant = "no", main = "NO vs Time for SN45")
timePlot(selectByDate(sn45, start = "6/4/2020", end = "4/5/2020"), pollutant = "no", main = "NO vs Time for SN45")
timePlot(selectByDate(sn45, start = "8/5/2020", end = "18/5/2020"), pollutant = "no", main = "NO vs Time for SN45")

```
```{r}
plotNOTrends <- function(sn, snstring)
  {
    pdf(paste0(snstring, "period1-no", ".pdf"))
    timePlot(sn[1:((1/5)*length(sn$date)),], pollutant = "no", main = snstring , y.relation = "free")
    dev.off()
    pdf(paste0(snstring, "period2-no", ".pdf"))
    timePlot(sn[((1/5)*length(sn$date)): ((2/5)*length(sn$date)),], pollutant ="no",  main = snstring ,y.relation = "free")
    dev.off()
    pdf(paste0(snstring, "period3-no", ".pdf"))
    timePlot(sn[((2/5)*length(sn$date)):((3/5)*length(sn$date)),], pollutant ="no",  main = snstring , y.relation = "free")
    dev.off()
    pdf(paste0(snstring, "period4-no", ".pdf"))
    timePlot(sn[((3/5)*length(sn$date)): ((4/5)*length(sn$date)),], pollutant = "no", main = snstring , y.relation = "free")
    dev.off()
    pdf(paste0(snstring, "period5-no", ".pdf"))
    timePlot(sn[((4/5)*length(sn$date)):length(sn$date),], pollutant ="no",  main = snstring , y.relation = "free")
    dev.off()
  
}
plotNOTrends(sn45, "sn45-before")
```

```{r}
sn45 %>%
summarise( 
          count = n(), 
          count_negative_no = sum(no < 0 ),
          count_negative_no2 = sum(no2model < 0 ), 
          count_negative_co2 = sum(co2 < 0 ), 
          count_negative_co = sum(co < 0), 
          count_negative_o3 = sum(o3 < 0 , na.rm = TRUE), 
          count_negative_pm1 = sum(pm1 < 0, na.rm = TRUE), 
          count_negative_pm25 = sum(pm25 <0 , na.rm = TRUE), 
          count_negative_pm10 = sum(pm10 < 0, na.rm = TRUE)
          )
```


# Cleaning O3 data 

Again, this is just taking the negative data and turning it to zero. 

```{r}
sn45$o3 <- replace(sn45$o3, sn45$o3 < 0, 0)
```

# Remove CO2 negative data 

There is one negative CO2 datapoint, according to the before graphs.

```{r}
sn45$co2 <- replace(sn45$co2, sn45$co2 < 0, 0)
```


# Creating an no_ae filter 

In order to determine the magnitude of the no_ae derivative to filter by, let's look at the average derivative and std 


```{r}
no_ae_filter <- function(sn, mag){
  #create a vector that shows the derivatives 
  no_ae_derivative <- c(0,diff(sn$no_ae, na.rm = TRUE))
  #creating logical vector to pick which things to get rid of 
  logical_vec <- abs(no_ae_derivative) < abs(mag*(sd(no_ae_derivative, na.rm=TRUE)))

  # if logical_vec is true, then set the NO value to NA 
  sn$no <- sn$no * logical_vec
  replace(sn$no, 0, NA)
  
  return(sn)

} 

```

```{r}
sn45_NOfilter_3SD <- no_ae_filter(sn45, 3)
```


```{r}

timePlot(selectByDate(sn45_NOfilter_3SD, start = "14/2/2020", end = "15/2/2020"), pollutant = "no", main = "NO vs Time for SN45")
timePlot(selectByDate(sn45_NOfilter_3SD, start = "24/2/2020", end = "29/2/2020"), pollutant = "no", main = "NO vs Time for SN45")
timePlot(selectByDate(sn45_NOfilter_3SD, start = "6/4/2020", end = "4/5/2020"), pollutant = "no", main = "NO vs Time for SN45")
timePlot(selectByDate(sn45_NOfilter_3SD, start = "8/5/2020", end = "18/5/2020"), pollutant = "no", main = "NO vs Time for SN45")
```

These particular graphs don't look different. 

```{r}

sn45_NOfilter_3SD %>%
summarise( 
          count = n(), 
          count_negative_no = sum(no < 0 , na.rm=TRUE),
          count_negative_no2 = sum(no2model < 0 ), 
          count_negative_co2 = sum(co2 < 0 ), 
          count_negative_co = sum(co < 0), 
          count_negative_o3 = sum(o3 < 0 , na.rm = TRUE)
          )

```
So, originally, there was 132529 negative NO points, but now there's about 500 points less. 

Let's see what happens when we reduce it to 2.5 sd 

```{r}

sn45_NOfilter_25 <- no_ae_filter(sn45, 2.5)

sn45_NOfilter_25 %>%
summarise( 
          count = n(), 
          count_negative_no = sum(no < 0 , na.rm=TRUE),
          count_negative_o3 = sum(o3 < 0 , na.rm = TRUE)
          )

```
This reduces it by 564 negative datapoints. Let's see if we can spot a difference graphically: 

```{r}

plotNOTrendsinline <- function(sn, snstring)
  {
    timePlot(sn[1:((1/5)*length(sn$date)),], pollutant = "no", main = snstring , y.relation = "free")
    timePlot(sn[((1/5)*length(sn$date)): ((2/5)*length(sn$date)),], pollutant ="no",  main = snstring ,y.relation = "free")
    timePlot(sn[((2/5)*length(sn$date)):((3/5)*length(sn$date)),], pollutant ="no",  main = snstring , y.relation = "free")
    timePlot(sn[((3/5)*length(sn$date)): ((4/5)*length(sn$date)),], pollutant = "no", main = snstring , y.relation = "free")
    timePlot(sn[((4/5)*length(sn$date)):length(sn$date),], pollutant ="no",  main = snstring , y.relation = "free")
  
}
plotNOTrendsinline(sn45_NOfilter_25, "Filtered NO vs Time - 2.5 *standard deviation ")

```

I do see some differences in these plots! Some negative outliers were removed. 

Let's see what 2 standard deviation does: 

```{r}


sn45_NOfilter_2 <- no_ae_filter(sn45, 2)

sn45_NOfilter_2 %>%
summarise( 
          count = n(), 
          count_negative_no = sum(no < 0 , na.rm=TRUE),
          count_negative_o3 = sum(o3 < 0 , na.rm = TRUE)
          )

plotNOTrendsinline(sn45_NOfilter_2, "Filtered NO vs Time - 2*Standard deviation ")
plotNOTrends(sn45_NOfilter_2, "FilteredNO_2SD")
```
907 negative NO points were removed. It does look like more outliers have been taken out. 

# Downsampling 

For now, I'll just run the function I created earlier, that takes the original datafiles in and downsamples them. 

```{r}
source("functions/creating5minresdf.R")
sn45_5min <- creating5minresdf(sensorpath = "Final_Customer/20200519_final_045.csv", asospath = "data/metdata.csv")

no2model <- read.csv("data/no2_HYBRID_045.txt") 
  # the timestamp is given as seconds from some origin, so I'll deduce the origin
no2model$date <-  as_datetime(no2model$timestamp , origin = (min(sn45_5min$originaldate) - min(no2model$timestamp))) 

no2model$date5min <- round_date(no2model$date, unit = "5 mins")   #to merge with finaldf
no2model$timestamp <- NULL #excluding this variable
names(no2model)[names(no2model)=="no2"] <- "no2model"#renaming, for the merge
no2model<- aggregate(no2model[,names(no2model) != "date5min"], by = list(no2model$date5min), FUN = mean, na.rm = TRUE) #takes mean of everything based on date5min

sn45_5min_complete <- merge(x=sn45_5min, y = no2model, by.x = "date", by.y = "Group.1", all=FALSE)

```



## Testing the effect of downsampling 

Rerunning the code to generate the before images 

```{r}
timePlot(selectByDate(sn45_5min_complete, start = "20/2/2020", end = "5/3/2020"), pollutant = "o3", main = "O3 vs Time for SN45")
timePlot(selectByDate(sn45_5min_complete, start = "30/11/2019", end = "15/12/2019"), pollutant = "co", main = "co vs Time for SN45")
timePlot(selectByDate(sn45_5min_complete, start = "14/2/2020", end = "15/2/2020"), pollutant = "no", main = "NO vs Time for SN45")
timePlot(selectByDate(sn45_5min_complete, start = "24/2/2020", end = "29/2/2020"), pollutant = "no", main = "NO vs Time for SN45")
timePlot(selectByDate(sn45_5min_complete, start = "6/4/2020", end = "4/5/2020"), pollutant = "no", main = "NO vs Time for SN45")
timePlot(selectByDate(sn45_5min_complete, start = "8/5/2020", end = "18/5/2020"), pollutant = "no", main = "NO vs Time for SN45")
```

It doesn't look like it got rid of the negative trends, because those trends span days (too long for downsampling to filter out). 

# Setting negative NO trends to zero 

In this section, I try to answer "How do the negative NO trends compare to the rest of the data?", "Is setting them to zero enough/appropriate for our analysis?" and "What other options exist?" 


In this section, I'll be using sn45 data that is filtered by no_ae already. I'll also remove duplicates, because I discovered that SN45 has 114 duplicate rows. 


```{r}
sn45_NOfilter_3SD <- sn45_NOfilter_3SD[!duplicated(sn45_NOfilter_3SD), ] #remove duplicates
(sd(sn45_NOfilter_3SD$no, na.rm= TRUE)) # print standard deviation of NO 
```

Now let's look at the statistics on the negative NO data: 

```{r}
negative_NO <- sn45_NOfilter_3SD$no[sn45_NOfilter_3SD$no < 0]

(length(negative_NO))
```
How many negative NO points are there, compared to all the data points? 

```{r}
(length(negative_NO) / length(sn45_NOfilter_3SD$no))
```

So, almost half of the datapoints are negative. However, are the all within the standard deviation of NO? within the standard error? Note, I'm not sure what the standard error of the NO sensor is, but I'll assume it's close to the CO2 one, which was 20 ppm.

```{r}
min(negative_NO, na.rm= TRUE)
```
It's about twice the limit of CO2's standard error.

The average NO value is:

```{r}
mean(negative_NO, na.rm=TRUE)
```
That's a lot closer to zero! 


Let's see what this data looks like 

```{r}
sn45_NOfilter_3SD$filteredNO <- replace(sn45_NOfilter_3SD$no, which(sn45_NOfilter_3SD$no < 0), NA)
timePlot(sn45_NOfilter_3SD, pollutant=c("filteredNO", "no"))

```
Hm, yeah I guess it's not really affecting the data that we care about, except for showing what happens long term . 

Let's see where the subtracted values are, and what they look like:

```{r}
sn45_NOfilter_3SD$filteredNOPOS <- replace(sn45_NOfilter_3SD$no, which(sn45_NOfilter_3SD$no > 0), 0)
timePlot(sn45_NOfilter_3SD, pollutant=c("filteredNOPOS", "no"))
timePlot(selectByDate(sn45_NOfilter_3SD, start = "6/9/2019", end="30/9/2019"), pollutant=c("filteredNOPOS", "no"))
timePlot(selectByDate(sn45_NOfilter_3SD, start = "6/9/2019", end="18/9/2019"), pollutant=c("filteredNOPOS", "no"))
timePlot(selectByDate(sn45_NOfilter_3SD, start = "12/9/2019", end="18/9/2019"), pollutant=c("filteredNOPOS", "no"))

```
And for other periods of time: 

```{r}
#random
timePlot(selectByDate(sn45_NOfilter_3SD, start = "4/12/2019", end="18/12/2019"), pollutant=c("filteredNOPOS", "no"))

#from before plots
timePlot(selectByDate(sn45_NOfilter_3SD, start = "14/2/2020", end = "15/2/2020"), pollutant = c("filteredNOPOS", "no"), main = "NO vs Time for SN45")
timePlot(selectByDate(sn45_NOfilter_3SD, start = "24/2/2020", end = "29/2/2020"), pollutant = c("filteredNOPOS", "no"), main = "NO vs Time for SN45")
timePlot(selectByDate(sn45_NOfilter_3SD, start = "6/4/2020", end = "4/5/2020"), pollutant = c("filteredNOPOS", "no"), main = "NO vs Time for SN45")
timePlot(selectByDate(sn45_NOfilter_3SD, start = "8/5/2020", end = "18/5/2020"), pollutant = c("filteredNOPOS", "no"), main = "NO vs Time for SN45")
```



We can see that sometimes, the negative values are just data whose baseline was shifted just a little below zero. Other times, trends are lost. 


For one, I think it would be okay to take everything that's < 2 ppb, and set it to zero. That way, we're approximating what the value actually is without losing as much resolution. 

The number of NO values that are between 0 and -2 are:
```{r}
sum(abs(negative_NO) < 2 , na.rm=TRUE )
```

Compared to all the negative NO values: 
```{r}
sum(abs(negative_NO) < 2 , na.rm=TRUE ) / length(negative_NO)
```
~34% of all the negative vales are less than -2 ppb. 
If we'd set those to zero instead of NA, we'd get back about a third of the points lost.

## Polynomial Fitting 

One technique that I found online, that I'd like to try out is polynomial fitting. 

### Using the baseline package

Trying the examples from the package info and stackoverflow: 

```{r}
library(baseline)
data(milk)
foo <- data.frame(Date=seq.Date(as.Date("1957-01-01"), by = "day", 
                            length.out = ncol(milk$spectra)),
              Visits=milk$spectra[1,],
              Old_baseline_visits=milk$spectra[1,], row.names = NULL)
foo.t <- t(foo$Visits) # Visits in a single row matrix 
bc.fillPeaks <- baseline(foo.t, lambda=6,
                     hwi=50, it=10, int=2000, method='fillPeaks')
plot(bc.fillPeaks)
```
```{r}
data(milk)
# The baseline() function is an S4 wrapper for all the different 
# baseline correction methods. The default correction method
# is IRLS. Data must be organized as row vectors in a matrix
# or data.frame.
bc.irls <- baseline(milk$spectra[1,, drop=FALSE])
## Not run: 
  # Computationally heavy
	plot(bc.irls)

## End(Not run)

# Available extractors are:
# getBaseline(bc.irls)
# getSpectra(bc.irls)
# getCorrected(bc.irls)
# getCall(bc.irls)

# Correction methods and parameters can be specified through the wrapper.
bc.fillPeaks <- baseline(milk$spectra[1,, drop=FALSE], lambda=6,
	hwi=50, it=10, int=2000, method='fillPeaks')
## Not run: 
  # Computationally heavy
	plot(bc.fillPeaks)
```
Ah, it looks like it will only do this for spectra. I looked at the code a little, but I might want to just try polynomial fitting before coming back to this. 


### Subtracting the polynomila manually

Let's see if we can subtract the polynomial too 

```{r}
polynom <- lm(negative_NO ~ poly(seq(1,length(negative_NO), 1), 3, raw=TRUE))

plot(negative_NO)
lines(seq(1,length(negative_NO), 1), predict(polynom, data.frame(x=seq(1,length(negative_NO), 1))), col='blue')
```

Hm, perhaps it could do it better if it's in sections?

Below, I'll try to import the date as well, and calculate the polynomial. This way, when it's time, it'll be easier to incorporate back in. 

```{r}
NO_Date <- sn45_NOfilter_3SD[sn45_NOfilter_3SD$no < 0, c("date", "no")] #extracting the date and NO
```

```{r}
#preparing the data for polynomial fit
NO_Date <- na.omit(NO_Date) #omitting NA
NO_Date <- NO_Date[!duplicated(NO_Date$date), ] #removing duplicates
rownames(NO_Date) <- NO_Date$date #setting date to the rownames
NO_Date$date <- as.numeric(NO_Date$date) #converting date to numeric to be able to use as x axis for polynomial model
polynom <- lm(NO_Date$no ~ poly(NO_Date$date, 3, raw=TRUE)) #create model

plot(NO_Date$date, NO_Date$no) #plot NO data
lines(NO_Date$no, predict(polynom, data.frame(x=NO_Date$date)), col='red', lw=4) #plot model
```


---
title: "Data Check"
output:
  html_document:
    df_print: paged
---

# Import Libraries

```{r, warning= FALSE, error = FALSE}
library(lubridate) #date and time functions 
library(data.table) #to use the data.table variable type
library(dplyr) #this library allows you to use the %>% operator
library(tidyr) #this library lets you use the complete function to account for time syncing
library(openair) #for plotting and analysis
library(stringr)
library(baseline) # for baseline correction
library(ggplot2)
```

# Import Files 

We'll import the files generated from the initial walkthrough file: 

```{r}
nm <- list.files(path = "./", pattern = "*.csv", full.names = TRUE)
snfiles <-  sapply(nm,  FUN = function(x) fread(file=x,  data.table = TRUE), simplify = FALSE,USE.NAMES = TRUE)
```
And rename them to just include "SN" and their sensor number. We'll do this by just snipping out the part we need from the current names, given by their file names.
```{r}
names(snfiles) <- lapply(names(snfiles), function(x) substr(x, 3, 6)) 
```

We'll set the date vector to be a date object:
```{r}
set_vector <- function(snfile){
  snfile$date_local <- ymd_hms(snfile$date_local, tz = "America/new_York")
  snfile$date <- ymd_hms(snfile$date, tz = "UTC")
  return(snfile)
}
snfiles <- sapply(snfiles, function(x) set_vector(x), simplify = FALSE,USE.NAMES = TRUE)
```

# Check data summaries

We'll create a function that prints out how many values for each pollutant in each sensor dataframe are NA, zero or negative.
```{r}
data_check_test <- function(sensor){
  
  temp_df <- sensor[, c( "co", "no" , "correctedNO", "no2", "o3" ,  "pm1" , "co2",  "bin0") ] #define which variables to inspect
  
  na_count <- apply(temp_df, 2,  function(x) sum(is.na(x))) #count the number of NA values
  
  negative_count <-apply(temp_df, 2,  function(x) sum(x<0, na.rm = TRUE)) #count the number of negative values
  
  zero_count <- apply(temp_df, 2,  function(x) sum(x==0, na.rm = TRUE)) #count the number of zero values
  
  return(data.frame(na_count, negative_count, zero_count))
}
```

```{r}
lapply(snfiles, function(x) data_check_test(x))
```

Next, we'll print out the 25% and 75% percentiles for each pollutant in each sensor dataframe. We'll also print out the mean for each pollutant in each sensor dataframe. 
```{r}
data_check_percentiles <- function(sensor){
  
  temp_df <- sensor[, c( "co", "no" ,  "correctedNO", "no2", "o3" ,  "pm1" , "co2","bin0") ] #define which variables to inspect
  
  percentiles <-apply(temp_df, 2,  function(x) quantile(x, probs =c(0.25, 0.75), na.rm = TRUE)) # apply the quantile function, which calculates percentiles
  
  return(percentiles)
}
data_check_mean <- function(sensor){
  
  temp_df <- sensor[, c( "co", "no" , "correctedNO",  "no2", "o3" ,  "pm1" , "co2",  "bin0") ] #define which variables to inspect
  
  mean_vals <-apply(temp_df, 2,  function(x) mean(x, na.rm = TRUE)) #apply the mean function
  
  return(mean_vals)
}
```

```{r}
lapply(snfiles, function(x) data_check_percentiles(x))
lapply(snfiles, function(x) data_check_mean(x))
```

# Plot Data 

Finally, we'll plot the time series of each sensor, in order to easily see where there are long periods with no data. 
The plots will get exported to a pdf.


```{r}
# pdf("time-plots.pdf")
# for (i in seq(length(names(snfiles)))){
# 
#   timePlot(snfiles[[i]], pollutant = c("no2", "co", "correctedNO", "bin0", "pm1"),y.relation = "free", main = paste0(names(snfiles)[i], "Time Plots"))
# }
# dev.off()
```

We'll also plot the meteorological temperature and quantAQ temperature together to see if they line up. 


```{r}
library(plotly)
test <- snfiles$sn45[, c(4,28,35)]
test <- na.omit(test)

test2 <- melt(test, id.var='date')
test2 <- na.omit(test2)

plot1 <- ggplot(test2, aes(x=date, y=value, color=variable)) + 
  geom_line() +
  scale_color_manual(values=c('red', 'green'))

ggplotly(plot1)
```

